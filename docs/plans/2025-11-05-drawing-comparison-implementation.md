# å›¾çº¸æ¯”å¯¹åŠŸèƒ½å®æ–½è®¡åˆ’

> **For Claude:** REQUIRED SUB-SKILL: Use superpowers:executing-plans to implement this plan task-by-task.

**ç›®æ ‡:** å®ç°æ™ºèƒ½å›¾çº¸ç‰ˆæœ¬å¯¹æ¯”åŠŸèƒ½ï¼Œè‡ªåŠ¨è¯†åˆ«V1/V2ç‰ˆæœ¬å·®å¼‚å¹¶ç”Ÿæˆå¯è§†åŒ–æ ‡æ³¨å’Œç»“æ„åŒ–å˜æ›´æ¸…å•

**æ¶æ„:** 4å±‚åˆ†ç¦»æ¶æ„ - Reactå‰ç«¯(Canvasæ ‡æ³¨) + Node.jsåç«¯(WebSocketæ¨é€) + Python AIæœåŠ¡(OpenCV+OCR+Qwen-VL) + æ•°æ®åº“å­˜å‚¨

**æŠ€æœ¯æ ˆ:** React 18 + Ant Design 5 + Canvas API + Node.js + Express + Bull Queue + Socket.io + Python 3.9 + OpenCV + Deepseek-OCR + Qwen-VL + PostgreSQL + Redis + MinIO

---

## ç¬¬ä¸€å‘¨ï¼šåŸºç¡€åŠŸèƒ½

### Task 1: æ•°æ®åº“è¡¨åˆ›å»º

**æ–‡ä»¶:**
- Create: `apps/api/src/database/migrations/20251105120000_create_drawing_comparison_tasks.js`

**Step 1: åˆ›å»ºè¿ç§»æ–‡ä»¶**

```javascript
exports.up = function(knex) {
  return knex.schema.createTable('drawing_comparison_tasks', function(table) {
    table.increments('id').primary();
    table.string('task_id', 50).notNullable().unique();
    table.string('user_id', 50).notNullable();
    table.string('project_id', 50);

    // æ–‡ä»¶URL
    table.text('v1_file_url').notNullable();
    table.text('v2_file_url').notNullable();
    table.text('annotated_image_url');

    // ä»»åŠ¡çŠ¶æ€
    table.string('status', 20).notNullable().defaultTo('pending');
    table.integer('progress').defaultTo(0);
    table.string('current_step', 100);
    table.text('error_message');

    // ç»“æœæ•°æ®ï¼ˆJSONæ ¼å¼ï¼‰
    table.jsonb('differences_json');

    // æ—¶é—´æˆ³
    table.timestamp('created_at').defaultTo(knex.fn.now());
    table.timestamp('updated_at').defaultTo(knex.fn.now());
    table.timestamp('completed_at');

    // ç´¢å¼•
    table.index('task_id');
    table.index('user_id');
    table.index('status');
  });
};

exports.down = function(knex) {
  return knex.schema.dropTable('drawing_comparison_tasks');
};
```

**Step 2: è¿è¡Œè¿ç§»**

```bash
cd apps/api
NODE_ENV=development npx knex migrate:latest
```

æœŸæœ›è¾“å‡ºï¼š
```
Batch 1 run: 1 migrations
migration file "20251105120000_create_drawing_comparison_tasks.js" successfully
```

**Step 3: éªŒè¯è¡¨åˆ›å»º**

```bash
PGPASSWORD=postgres psql -h localhost -p 5433 -U postgres -d design_platform -c "\d drawing_comparison_tasks"
```

æœŸæœ›è¾“å‡ºï¼šæ˜¾ç¤ºè¡¨ç»“æ„

**Step 4: æäº¤**

```bash
git add apps/api/src/database/migrations/20251105120000_create_drawing_comparison_tasks.js
git commit -m "feat(db): add drawing_comparison_tasks table

- åˆ›å»ºå›¾çº¸æ¯”å¯¹ä»»åŠ¡è¡¨
- æ”¯æŒä»»åŠ¡çŠ¶æ€è·Ÿè¸ª
- JSONBå­˜å‚¨å·®å¼‚ç»“æœ

ğŸ¤– Generated with [Claude Code](https://claude.com/claude-code)

Co-Authored-By: Claude <noreply@anthropic.com>"
```

---

### Task 2: æ·»åŠ èœå•é…ç½®

**æ–‡ä»¶:**
- Create: `apps/api/src/database/migrations/20251105120100_add_drawing_comparison_menu.js`

**Step 1: åˆ›å»ºèœå•è¿ç§»**

```javascript
exports.up = async function(knex) {
  // æ’å…¥å›¾çº¸æ¯”å¯¹èœå•
  await knex('menus').insert({
    id: knex.raw('gen_random_uuid()::text'),
    name: 'å›¾çº¸æ¯”å¯¹',
    code: 'drawing_comparison',
    path: '/mechanical-design/drawing-comparison',
    component: 'DrawingComparison',
    icon: 'DiffOutlined',
    parent_id: 'fb094603-8855-43d5-9e86-b46cb46c5c7b', // æœºæ¢°è®¾è®¡çˆ¶èœå•ID
    type: 'menu',
    sort_order: 5,
    status: 'active',
    visible: true,
    permission_code: 'mechanical:drawing:comparison',
    permissions: JSON.stringify([
      { action: 'view', name: 'æŸ¥çœ‹å›¾çº¸æ¯”å¯¹' },
      { action: 'compare', name: 'æ‰§è¡Œæ¯”å¯¹' },
      { action: 'export', name: 'å¯¼å‡ºæŠ¥å‘Š' }
    ])
  });
};

exports.down = function(knex) {
  return knex('menus').where('code', 'drawing_comparison').del();
};
```

**Step 2: è¿è¡Œè¿ç§»**

```bash
NODE_ENV=development npx knex migrate:latest
```

**Step 3: éªŒè¯èœå•åˆ›å»º**

```bash
PGPASSWORD=postgres psql -h localhost -p 5433 -U postgres -d design_platform -c "SELECT id, name, path, parent_id FROM menus WHERE code = 'drawing_comparison';"
```

**Step 4: æäº¤**

```bash
git add apps/api/src/database/migrations/20251105120100_add_drawing_comparison_menu.js
git commit -m "feat(menu): add drawing comparison menu item

- æ·»åŠ å›¾çº¸æ¯”å¯¹èœå•åˆ°æœºæ¢°è®¾è®¡æ¨¡å—
- é…ç½®æƒé™ï¼šæŸ¥çœ‹ã€æ¯”å¯¹ã€å¯¼å‡º

ğŸ¤– Generated with [Claude Code](https://claude.com/claude-code)

Co-Authored-By: Claude <noreply@anthropic.com>"
```

---

### Task 3: åç«¯æœåŠ¡å±‚ - DrawingComparisonService

**æ–‡ä»¶:**
- Create: `apps/api/src/services/drawing/DrawingComparisonService.js`

**Step 1: ç¼–å†™æœåŠ¡åŸºç¡€ç»“æ„æµ‹è¯•**

Create: `apps/api/tests/services/DrawingComparisonService.test.js`

```javascript
const DrawingComparisonService = require('../../src/services/drawing/DrawingComparisonService');
const knex = require('../../src/database/knex');

describe('DrawingComparisonService', () => {
  afterAll(async () => {
    await knex.destroy();
  });

  describe('createTask', () => {
    it('should create a comparison task', async () => {
      const taskData = {
        userId: 'user_test',
        v1FileUrl: 'minio://v1.pdf',
        v2FileUrl: 'minio://v2.pdf',
        projectId: 'proj_123'
      };

      const task = await DrawingComparisonService.createTask(taskData);

      expect(task).toHaveProperty('taskId');
      expect(task.status).toBe('pending');
      expect(task.progress).toBe(0);
    });
  });
});
```

**Step 2: è¿è¡Œæµ‹è¯•ç¡®è®¤å¤±è´¥**

```bash
npm test -- DrawingComparisonService.test.js
```

æœŸæœ›è¾“å‡ºï¼š`Cannot find module '../../src/services/drawing/DrawingComparisonService'`

**Step 3: å®ç°æœåŠ¡**

Create: `apps/api/src/services/drawing/DrawingComparisonService.js`

```javascript
const knex = require('../../database/knex');
const { v4: uuidv4 } = require('uuid');

class DrawingComparisonService {
  /**
   * åˆ›å»ºå›¾çº¸æ¯”å¯¹ä»»åŠ¡
   */
  static async createTask({ userId, v1FileUrl, v2FileUrl, projectId, description }) {
    const taskId = `cmp_${Date.now()}_${uuidv4().substring(0, 8)}`;

    const [task] = await knex('drawing_comparison_tasks')
      .insert({
        task_id: taskId,
        user_id: userId,
        project_id: projectId,
        v1_file_url: v1FileUrl,
        v2_file_url: v2FileUrl,
        status: 'pending',
        progress: 0,
        created_at: knex.fn.now(),
        updated_at: knex.fn.now()
      })
      .returning('*');

    return {
      taskId: task.task_id,
      status: task.status,
      progress: task.progress,
      createdAt: task.created_at
    };
  }

  /**
   * æ›´æ–°ä»»åŠ¡çŠ¶æ€
   */
  static async updateTaskStatus(taskId, { status, progress, currentStep, errorMessage }) {
    const updateData = {
      updated_at: knex.fn.now()
    };

    if (status) updateData.status = status;
    if (typeof progress === 'number') updateData.progress = progress;
    if (currentStep) updateData.current_step = currentStep;
    if (errorMessage) updateData.error_message = errorMessage;
    if (status === 'completed') updateData.completed_at = knex.fn.now();

    await knex('drawing_comparison_tasks')
      .where('task_id', taskId)
      .update(updateData);
  }

  /**
   * è·å–ä»»åŠ¡è¯¦æƒ…
   */
  static async getTask(taskId) {
    const task = await knex('drawing_comparison_tasks')
      .where('task_id', taskId)
      .first();

    if (!task) {
      throw new Error(`Task ${taskId} not found`);
    }

    return {
      taskId: task.task_id,
      userId: task.user_id,
      projectId: task.project_id,
      status: task.status,
      progress: task.progress,
      currentStep: task.current_step,
      errorMessage: task.error_message,
      v1FileUrl: task.v1_file_url,
      v2FileUrl: task.v2_file_url,
      annotatedImageUrl: task.annotated_image_url,
      differences: task.differences_json,
      createdAt: task.created_at,
      completedAt: task.completed_at
    };
  }

  /**
   * ä¿å­˜æ¯”å¯¹ç»“æœ
   */
  static async saveResult(taskId, { annotatedImageUrl, differences }) {
    await knex('drawing_comparison_tasks')
      .where('task_id', taskId)
      .update({
        annotated_image_url: annotatedImageUrl,
        differences_json: JSON.stringify(differences),
        status: 'completed',
        progress: 100,
        completed_at: knex.fn.now(),
        updated_at: knex.fn.now()
      });
  }

  /**
   * è·å–ç”¨æˆ·çš„ä»»åŠ¡åˆ—è¡¨
   */
  static async getUserTasks(userId, { page = 1, pageSize = 20 }) {
    const offset = (page - 1) * pageSize;

    const tasks = await knex('drawing_comparison_tasks')
      .where('user_id', userId)
      .orderBy('created_at', 'desc')
      .limit(pageSize)
      .offset(offset);

    const [{ count }] = await knex('drawing_comparison_tasks')
      .where('user_id', userId)
      .count('* as count');

    return {
      tasks: tasks.map(task => ({
        taskId: task.task_id,
        status: task.status,
        progress: task.progress,
        createdAt: task.created_at,
        completedAt: task.completed_at
      })),
      total: parseInt(count),
      page,
      pageSize
    };
  }
}

module.exports = DrawingComparisonService;
```

**Step 4: è¿è¡Œæµ‹è¯•ç¡®è®¤é€šè¿‡**

```bash
npm test -- DrawingComparisonService.test.js
```

æœŸæœ›è¾“å‡ºï¼š`PASS  1 test passed`

**Step 5: æäº¤**

```bash
git add apps/api/src/services/drawing/DrawingComparisonService.js apps/api/tests/services/DrawingComparisonService.test.js
git commit -m "feat(service): add DrawingComparisonService

- åˆ›å»ºä»»åŠ¡ç®¡ç†
- çŠ¶æ€æ›´æ–°
- ç»“æœä¿å­˜
- TDDå®ç°

ğŸ¤– Generated with [Claude Code](https://claude.com/claude-code)

Co-Authored-By: Claude <noreply@anthropic.com>"
```

---

### Task 4: åç«¯æ§åˆ¶å™¨ - DrawingComparisonController

**æ–‡ä»¶:**
- Create: `apps/api/src/controllers/DrawingComparisonController.js`

**Step 1: åˆ›å»ºæ§åˆ¶å™¨**

```javascript
const DrawingComparisonService = require('../services/drawing/DrawingComparisonService');
const multer = require('multer');
const path = require('path');
const { v4: uuidv4 } = require('uuid');
const MinioService = require('../services/storage/MinioService');

// é…ç½®multerå­˜å‚¨
const storage = multer.diskStorage({
  destination: (req, file, cb) => {
    cb(null, 'uploads/temp/');
  },
  filename: (req, file, cb) => {
    const uniqueName = `${Date.now()}_${uuidv4()}${path.extname(file.originalname)}`;
    cb(null, uniqueName);
  }
});

const upload = multer({
  storage,
  limits: { fileSize: 50 * 1024 * 1024 }, // 50MBé™åˆ¶
  fileFilter: (req, file, cb) => {
    const allowedTypes = ['application/pdf', 'image/png', 'image/jpeg'];
    if (allowedTypes.includes(file.mimetype)) {
      cb(null, true);
    } else {
      cb(new Error('åªæ”¯æŒ PDFã€PNGã€JPG æ ¼å¼'));
    }
  }
});

class DrawingComparisonController {
  /**
   * åˆ›å»ºæ¯”å¯¹ä»»åŠ¡ï¼ˆä¸Šä¼ æ–‡ä»¶ï¼‰
   */
  static async createComparison(req, res) {
    try {
      const { projectId, description } = req.body;
      const userId = req.user.userId;

      // æ£€æŸ¥æ–‡ä»¶
      if (!req.files || !req.files.v1File || !req.files.v2File) {
        return res.status(400).json({
          success: false,
          message: 'è¯·ä¸Šä¼ V1å’ŒV2ä¸¤ä¸ªæ–‡ä»¶'
        });
      }

      const v1File = req.files.v1File[0];
      const v2File = req.files.v2File[0];

      // ä¸Šä¼ åˆ°MinIO
      const v1FileUrl = await MinioService.uploadFile(
        'drawing-comparison',
        v1File.path,
        `v1_${Date.now()}_${v1File.originalname}`
      );

      const v2FileUrl = await MinioService.uploadFile(
        'drawing-comparison',
        `v2_${Date.now()}_${v2File.originalname}`,
        v2File.path
      );

      // åˆ›å»ºä»»åŠ¡
      const task = await DrawingComparisonService.createTask({
        userId,
        v1FileUrl,
        v2FileUrl,
        projectId,
        description
      });

      // å¼‚æ­¥è°ƒç”¨PythonæœåŠ¡è¿›è¡Œåˆ†æï¼ˆåç»­Taskå®ç°ï¼‰
      // TODO: å°†ä»»åŠ¡åŠ å…¥Bullé˜Ÿåˆ—

      res.json({
        success: true,
        data: task,
        message: 'ä»»åŠ¡åˆ›å»ºæˆåŠŸï¼Œå¼€å§‹å¤„ç†'
      });
    } catch (error) {
      console.error('åˆ›å»ºæ¯”å¯¹ä»»åŠ¡å¤±è´¥:', error);
      res.status(500).json({
        success: false,
        message: error.message
      });
    }
  }

  /**
   * è·å–ä»»åŠ¡çŠ¶æ€
   */
  static async getTaskStatus(req, res) {
    try {
      const { taskId } = req.params;
      const task = await DrawingComparisonService.getTask(taskId);

      res.json({
        success: true,
        data: {
          taskId: task.taskId,
          status: task.status,
          progress: task.progress,
          currentStep: task.currentStep,
          message: task.errorMessage
        }
      });
    } catch (error) {
      res.status(500).json({
        success: false,
        message: error.message
      });
    }
  }

  /**
   * è·å–æ¯”å¯¹ç»“æœ
   */
  static async getResult(req, res) {
    try {
      const { taskId } = req.params;
      const task = await DrawingComparisonService.getTask(taskId);

      if (task.status !== 'completed') {
        return res.status(400).json({
          success: false,
          message: 'ä»»åŠ¡å°šæœªå®Œæˆ'
        });
      }

      res.json({
        success: true,
        data: {
          taskId: task.taskId,
          status: task.status,
          v2ImageUrl: task.v2FileUrl,
          annotatedImageUrl: task.annotatedImageUrl,
          differences: task.differences,
          summary: {
            totalDifferences: task.differences?.length || 0,
            byCategory: calculateCategoryStats(task.differences)
          }
        }
      });
    } catch (error) {
      res.status(500).json({
        success: false,
        message: error.message
      });
    }
  }

  /**
   * è·å–ç”¨æˆ·ä»»åŠ¡åˆ—è¡¨
   */
  static async getUserTasks(req, res) {
    try {
      const userId = req.user.userId;
      const { page = 1, pageSize = 20 } = req.query;

      const result = await DrawingComparisonService.getUserTasks(userId, {
        page: parseInt(page),
        pageSize: parseInt(pageSize)
      });

      res.json({
        success: true,
        data: result
      });
    } catch (error) {
      res.status(500).json({
        success: false,
        message: error.message
      });
    }
  }
}

// è¾…åŠ©å‡½æ•°ï¼šè®¡ç®—åˆ†ç±»ç»Ÿè®¡
function calculateCategoryStats(differences) {
  if (!differences || differences.length === 0) return {};

  return differences.reduce((acc, diff) => {
    const category = diff.category || 'unknown';
    acc[category] = (acc[category] || 0) + 1;
    return acc;
  }, {});
}

module.exports = {
  DrawingComparisonController,
  upload
};
```

**Step 2: æäº¤**

```bash
git add apps/api/src/controllers/DrawingComparisonController.js
git commit -m "feat(controller): add DrawingComparisonController

- æ–‡ä»¶ä¸Šä¼ å¤„ç†
- ä»»åŠ¡åˆ›å»º
- çŠ¶æ€æŸ¥è¯¢
- ç»“æœè·å–

ğŸ¤– Generated with [Claude Code](https://claude.com/claude-code)

Co-Authored-By: Claude <noreply@anthropic.com>"
```

---

### Task 5: åç«¯è·¯ç”±é…ç½®

**æ–‡ä»¶:**
- Create: `apps/api/src/routes/drawingComparison.js`
- Modify: `apps/api/src/app.js`

**Step 1: åˆ›å»ºè·¯ç”±æ–‡ä»¶**

```javascript
const express = require('express');
const router = express.Router();
const { DrawingComparisonController, upload } = require('../controllers/DrawingComparisonController');
const { authenticateToken } = require('../middleware/auth');

// æ‰€æœ‰è·¯ç”±éƒ½éœ€è¦è®¤è¯
router.use(authenticateToken);

// åˆ›å»ºæ¯”å¯¹ä»»åŠ¡ï¼ˆæ–‡ä»¶ä¸Šä¼ ï¼‰
router.post('/compare',
  upload.fields([
    { name: 'v1File', maxCount: 1 },
    { name: 'v2File', maxCount: 1 }
  ]),
  DrawingComparisonController.createComparison
);

// è·å–ä»»åŠ¡çŠ¶æ€
router.get('/status/:taskId', DrawingComparisonController.getTaskStatus);

// è·å–æ¯”å¯¹ç»“æœ
router.get('/result/:taskId', DrawingComparisonController.getResult);

// è·å–ç”¨æˆ·ä»»åŠ¡åˆ—è¡¨
router.get('/tasks', DrawingComparisonController.getUserTasks);

module.exports = router;
```

**Step 2: æ³¨å†Œè·¯ç”±åˆ°app.js**

Modify: `apps/api/src/app.js`

åœ¨ç°æœ‰è·¯ç”±æ³¨å†Œåæ·»åŠ ï¼š

```javascript
// å›¾çº¸æ¯”å¯¹è·¯ç”±
const drawingComparisonRoutes = require('./routes/drawingComparison');
app.use('/api/drawing-comparison', drawingComparisonRoutes);
```

**Step 3: æµ‹è¯•è·¯ç”±æ³¨å†Œ**

```bash
# å¯åŠ¨æœåŠ¡å™¨
PORT=3000 node apps/api/src/app.js &

# æµ‹è¯•å¥åº·æ£€æŸ¥
curl http://localhost:3000/api/system/health

# å…³é—­æœåŠ¡å™¨
pkill -f "node apps/api/src/app.js"
```

**Step 4: æäº¤**

```bash
git add apps/api/src/routes/drawingComparison.js apps/api/src/app.js
git commit -m "feat(routes): add drawing comparison routes

- POST /api/drawing-comparison/compare
- GET /api/drawing-comparison/status/:taskId
- GET /api/drawing-comparison/result/:taskId
- GET /api/drawing-comparison/tasks

ğŸ¤– Generated with [Claude Code](https://claude.com/claude-code)

Co-Authored-By: Claude <noreply@anthropic.com>"
```

---

## ç¬¬äºŒå‘¨ï¼šAIæœåŠ¡é›†æˆ

### Task 6: PythonæœåŠ¡é‡æ„ - æå–é€šç”¨æ¨¡å—

**æ–‡ä»¶:**
- Create: `services/document-recognition/common/__init__.py`
- Create: `services/document-recognition/common/ocr_client.py`
- Create: `services/document-recognition/common/vision_client.py`
- Create: `services/document-recognition/common/cv_utils.py`

**Step 1: åˆ›å»ºOCRå®¢æˆ·ç«¯**

```python
#!/usr/bin/env python3
"""
Deepseek-OCRç»Ÿä¸€è°ƒç”¨å®¢æˆ·ç«¯
å¤ç”¨è‡ªPIDè¯†åˆ«æœåŠ¡
"""
import os
import requests
import time
from typing import Dict, List

class OCRClient:
    def __init__(self):
        self.ocr_service_url = os.getenv(
            'DOCUMENT_RECOGNITION_SERVICE',
            'http://10.10.18.3:7000/ocr'
        )
        self.max_retries = 3
        self.timeout = 30

    def recognize(self, image_path: str, page_num: int = 0) -> List[Dict]:
        """
        è¯†åˆ«å›¾ç‰‡ä¸­çš„æ–‡å­—

        Returns:
            [{"text": "...", "bbox": [x, y, w, h], "confidence": 0.9}, ...]
        """
        for attempt in range(self.max_retries):
            try:
                with open(image_path, 'rb') as f:
                    response = requests.post(
                        self.ocr_service_url,
                        files={'file': f},
                        data={'page': page_num},
                        timeout=self.timeout
                    )

                response.raise_for_status()
                result = response.json()

                if result.get('success'):
                    return self._parse_ocr_result(result.get('data', []))
                else:
                    raise Exception(f"OCRå¤±è´¥: {result.get('message')}")

            except requests.Timeout:
                print(f"  âš ï¸  OCRè¶…æ—¶ï¼Œé‡è¯• {attempt+1}/{self.max_retries}")
                time.sleep(2)

            except requests.ConnectionError as e:
                if attempt == self.max_retries - 1:
                    print(f"  âŒ OCRæœåŠ¡ä¸å¯ç”¨: {e}")
                    return []
                time.sleep(2)

            except Exception as e:
                print(f"  âš ï¸  OCRé”™è¯¯: {e}")
                if attempt == self.max_retries - 1:
                    return []
                time.sleep(2)

        return []

    def _parse_ocr_result(self, data: List) -> List[Dict]:
        """è§£æOCRè¿”å›ç»“æœ"""
        parsed = []
        for item in data:
            parsed.append({
                'text': item.get('text', ''),
                'bbox': item.get('bbox', [0, 0, 0, 0]),
                'confidence': item.get('confidence', 0.0)
            })
        return parsed

# å•ä¾‹æ¨¡å¼
_ocr_client_instance = None

def get_ocr_client():
    global _ocr_client_instance
    if _ocr_client_instance is None:
        _ocr_client_instance = OCRClient()
    return _ocr_client_instance
```

**Step 2: åˆ›å»ºQwen-VLå®¢æˆ·ç«¯**

Create: `services/document-recognition/common/vision_client.py`

```python
#!/usr/bin/env python3
"""
Qwen-VLç»Ÿä¸€è°ƒç”¨å®¢æˆ·ç«¯
"""
import os
import base64
import requests
import json
from typing import Dict, List

class VisionLLMClient:
    def __init__(self):
        self.vl_url = os.getenv(
            'QWEN_VL_URL',
            'http://10.10.18.3:8001/v1/chat/completions'
        )
        self.model = os.getenv('QWEN_VL_MODEL', 'Qwen-VL')
        self.available = self._check_availability()

    def _check_availability(self) -> bool:
        """æ£€æŸ¥Qwen-VLæœåŠ¡æ˜¯å¦å¯ç”¨"""
        try:
            response = requests.get(
                self.vl_url.replace('/v1/chat/completions', '/v1/models'),
                timeout=5
            )
            if response.status_code == 200:
                print("âœ… Qwen-VLæœåŠ¡å¯ç”¨")
                return True
        except:
            print("âš ï¸  Qwen-VLæœåŠ¡æœªå¯åŠ¨ï¼ŒæŸäº›åŠŸèƒ½å°†é™çº§")
        return False

    def analyze(self, images: List[str], prompt: str) -> Dict:
        """
        ä½¿ç”¨Qwen-VLåˆ†æå›¾ç‰‡

        Args:
            images: å›¾ç‰‡è·¯å¾„åˆ—è¡¨
            prompt: æç¤ºè¯

        Returns:
            è§£æåçš„JSONç»“æœ
        """
        if not self.available:
            return {
                "error": "Qwen-VLæœåŠ¡ä¸å¯ç”¨",
                "fallback": True
            }

        # è½¬æ¢å›¾ç‰‡ä¸ºbase64
        image_contents = []
        for img_path in images:
            with open(img_path, 'rb') as f:
                img_b64 = base64.b64encode(f.read()).decode('utf-8')
                image_contents.append({
                    "type": "image_url",
                    "image_url": {
                        "url": f"data:image/png;base64,{img_b64}"
                    }
                })

        # æ„å»ºæ¶ˆæ¯
        messages = [{
            "role": "user",
            "content": [
                {"type": "text", "text": prompt},
                *image_contents
            ]
        }]

        # è°ƒç”¨API
        payload = {
            "model": self.model,
            "messages": messages,
            "temperature": 0.1,
            "max_tokens": 4000
        }

        try:
            response = requests.post(
                self.vl_url,
                json=payload,
                timeout=60
            )
            response.raise_for_status()

            result = response.json()
            content = result['choices'][0]['message']['content']

            return self._parse_json_response(content)

        except Exception as e:
            print(f"  âš ï¸  Qwen-VLè°ƒç”¨å¤±è´¥: {e}")
            return {"error": str(e), "fallback": True}

    def _parse_json_response(self, content: str) -> Dict:
        """è§£æLLMè¿”å›çš„JSON"""
        # æå–JSONéƒ¨åˆ†
        if '```json' in content:
            start = content.find('```json') + 7
            end = content.find('```', start)
            json_str = content[start:end].strip()
        elif '```' in content:
            start = content.find('```') + 3
            end = content.find('```', start)
            json_str = content[start:end].strip()
        else:
            json_str = content.strip()

        try:
            return json.loads(json_str)
        except json.JSONDecodeError as e:
            print(f"  âš ï¸  JSONè§£æå¤±è´¥: {e}")
            return {"error": "JSONè§£æå¤±è´¥", "raw": content}

# å•ä¾‹
_vision_client_instance = None

def get_vision_client():
    global _vision_client_instance
    if _vision_client_instance is None:
        _vision_client_instance = VisionLLMClient()
    return _vision_client_instance
```

**Step 3: åˆ›å»ºOpenCVå·¥å…·**

Create: `services/document-recognition/common/cv_utils.py`

```python
#!/usr/bin/env python3
"""
OpenCVé€šç”¨å·¥å…·
"""
import cv2
import numpy as np
from typing import List, Dict, Tuple

def align_images(img1: np.ndarray, img2: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
    """
    å›¾åƒé…å‡†ï¼šå°†img1å¯¹é½åˆ°img2

    Returns:
        (aligned_img1, transform_matrix)
    """
    # è½¬æ¢ä¸ºç°åº¦å›¾
    gray1 = cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY)
    gray2 = cv2.cvtColor(img2, cv2.COLOR_BGR2GRAY)

    # ORBç‰¹å¾æ£€æµ‹
    orb = cv2.ORB_create(5000)
    kp1, des1 = orb.detectAndCompute(gray1, None)
    kp2, des2 = orb.detectAndCompute(gray2, None)

    # ç‰¹å¾åŒ¹é…
    matcher = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)
    matches = matcher.match(des1, des2)
    matches = sorted(matches, key=lambda x: x.distance)

    # æ£€æŸ¥ç‰¹å¾ç‚¹æ•°é‡
    if len(matches) < 10:
        raise ValueError(
            f"ç‰¹å¾ç‚¹ä¸è¶³({len(matches)}ä¸ª)ï¼Œå¯èƒ½åŸå› ï¼š\n"
            "1. ä¸¤å¼ å›¾çº¸å·®å¼‚è¿‡å¤§\n"
            "2. å›¾çº¸è´¨é‡å¤ªå·®\n"
            "3. å›¾çº¸æ–¹å‘ä¸ä¸€è‡´"
        )

    # æå–åŒ¹é…ç‚¹
    src_pts = np.float32([kp1[m.queryIdx].pt for m in matches]).reshape(-1, 1, 2)
    dst_pts = np.float32([kp2[m.trainIdx].pt for m in matches]).reshape(-1, 1, 2)

    # è®¡ç®—å˜æ¢çŸ©é˜µ
    H, mask = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC, 5.0)

    if H is None or np.sum(mask) < 4:
        raise ValueError("é…å‡†å¤±è´¥ï¼šæ— æ³•è®¡ç®—æœ‰æ•ˆçš„å˜æ¢çŸ©é˜µ")

    # åº”ç”¨å˜æ¢
    height, width = img2.shape[:2]
    aligned = cv2.warpPerspective(img1, H, (width, height))

    return aligned, H


def detect_differences(img1: np.ndarray, img2: np.ndarray, threshold: int = 30) -> List[Dict]:
    """
    æ£€æµ‹ä¸¤å¼ å›¾ç‰‡çš„å·®å¼‚åŒºåŸŸ

    Returns:
        [{"x": 100, "y": 200, "width": 80, "height": 60}, ...]
    """
    # è®¡ç®—å·®å¼‚
    diff = cv2.absdiff(img1, img2)
    gray_diff = cv2.cvtColor(diff, cv2.COLOR_BGR2GRAY)

    # äºŒå€¼åŒ–
    _, thresh = cv2.threshold(gray_diff, threshold, 255, cv2.THRESH_BINARY)

    # å½¢æ€å­¦æ“ä½œï¼šå»å™ªå£°ã€è¿æ¥é‚»è¿‘åŒºåŸŸ
    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (5, 5))
    thresh = cv2.morphologyEx(thresh, cv2.MORPH_CLOSE, kernel)
    thresh = cv2.morphologyEx(thresh, cv2.MORPH_OPEN, kernel)

    # æŸ¥æ‰¾è½®å»“
    contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)

    # è¿‡æ»¤å¹¶æå–è¾¹ç•Œæ¡†
    regions = []
    for contour in contours:
        area = cv2.contourArea(contour)
        if area > 100:  # è¿‡æ»¤å°å™ªå£°
            x, y, w, h = cv2.boundingRect(contour)
            regions.append({
                "x": int(x),
                "y": int(y),
                "width": int(w),
                "height": int(h)
            })

    # åˆå¹¶é‚»è¿‘åŒºåŸŸ
    regions = merge_nearby_regions(regions, distance_threshold=50)

    return regions


def merge_nearby_regions(regions: List[Dict], distance_threshold: int = 50) -> List[Dict]:
    """åˆå¹¶é‚»è¿‘çš„å·®å¼‚åŒºåŸŸ"""
    if len(regions) <= 1:
        return regions

    merged = []
    used = set()

    for i, r1 in enumerate(regions):
        if i in used:
            continue

        # æŸ¥æ‰¾å¯åˆå¹¶çš„åŒºåŸŸ
        merge_group = [r1]
        for j, r2 in enumerate(regions[i+1:], start=i+1):
            if j in used:
                continue

            # è®¡ç®—ä¸­å¿ƒè·ç¦»
            c1_x = r1['x'] + r1['width'] / 2
            c1_y = r1['y'] + r1['height'] / 2
            c2_x = r2['x'] + r2['width'] / 2
            c2_y = r2['y'] + r2['height'] / 2

            distance = np.sqrt((c1_x - c2_x)**2 + (c1_y - c2_y)**2)

            if distance < distance_threshold:
                merge_group.append(r2)
                used.add(j)

        # åˆå¹¶
        if len(merge_group) == 1:
            merged.append(r1)
        else:
            min_x = min(r['x'] for r in merge_group)
            min_y = min(r['y'] for r in merge_group)
            max_x = max(r['x'] + r['width'] for r in merge_group)
            max_y = max(r['y'] + r['height'] for r in merge_group)

            merged.append({
                "x": int(min_x),
                "y": int(min_y),
                "width": int(max_x - min_x),
                "height": int(max_y - min_y)
            })

    return merged


def crop_with_context(img: np.ndarray, region: Dict, padding: float = 0.2) -> np.ndarray:
    """
    è£å‰ªåŒºåŸŸå¹¶æ·»åŠ ä¸Šä¸‹æ–‡

    Args:
        padding: æ‰©å±•æ¯”ä¾‹ï¼ˆ0.2 = æ‰©å¤§20%ï¼‰
    """
    h, w = img.shape[:2]

    x = region['x']
    y = region['y']
    rw = region['width']
    rh = region['height']

    # è®¡ç®—æ‰©å±•åçš„åŒºåŸŸ
    pad_w = int(rw * padding)
    pad_h = int(rh * padding)

    x1 = max(0, x - pad_w)
    y1 = max(0, y - pad_h)
    x2 = min(w, x + rw + pad_w)
    y2 = min(h, y + rh + pad_h)

    return img[y1:y2, x1:x2]
```

**Step 4: åˆ›å»º__init__.py**

Create: `services/document-recognition/common/__init__.py`

```python
from .ocr_client import get_ocr_client
from .vision_client import get_vision_client
from .cv_utils import align_images, detect_differences, crop_with_context

__all__ = [
    'get_ocr_client',
    'get_vision_client',
    'align_images',
    'detect_differences',
    'crop_with_context'
]
```

**Step 5: æäº¤**

```bash
git add services/document-recognition/common/
git commit -m "refactor(ai): extract common OCR/Vision/CV utilities

- OCRå®¢æˆ·ç«¯ç»Ÿä¸€å°è£…ï¼ˆæ”¯æŒé‡è¯•ï¼‰
- Qwen-VLå®¢æˆ·ç«¯ï¼ˆå¥åº·æ£€æŸ¥+é™çº§ï¼‰
- OpenCVå·¥å…·ï¼ˆé…å‡†+å·®å¼‚æ£€æµ‹+åŒºåŸŸåˆå¹¶ï¼‰
- å¯è¢«PIDè¯†åˆ«å’Œå›¾çº¸æ¯”å¯¹å…±äº«

ğŸ¤– Generated with [Claude Code](https://claude.com/claude-code)

Co-Authored-By: Claude <noreply@anthropic.com>"
```

---

### Task 7: å›¾çº¸æ¯”å¯¹AIæœåŠ¡

**æ–‡ä»¶:**
- Create: `services/document-recognition/drawing_comparison/__init__.py`
- Create: `services/document-recognition/drawing_comparison/diff_analyzer.py`
- Create: `services/document-recognition/drawing_comparison/report_generator.py`

**Step 1: åˆ›å»ºå·®å¼‚åˆ†æå™¨**

```python
#!/usr/bin/env python3
"""
å›¾çº¸å·®å¼‚åˆ†æå™¨
æ•´åˆOpenCVã€OCRã€Qwen-VLä¸‰é˜¶æ®µåˆ†æ
"""
import cv2
import numpy as np
from pathlib import Path
from typing import Dict, List
import sys
sys.path.append(str(Path(__file__).parent.parent))

from common import (
    get_ocr_client,
    get_vision_client,
    align_images,
    detect_differences,
    crop_with_context
)

class DrawingDiffAnalyzer:
    def __init__(self):
        self.ocr = get_ocr_client()
        self.vision = get_vision_client()

    def analyze(self, v1_path: str, v2_path: str) -> Dict:
        """
        å®Œæ•´åˆ†ææµç¨‹

        Returns:
            {
                "success": True,
                "differences": [...],
                "aligned_v1_path": "...",
                "method": "full" | "basic"
            }
        """
        print(f"ğŸ” å¼€å§‹å›¾çº¸æ¯”å¯¹åˆ†æ")
        print(f"  V1: {Path(v1_path).name}")
        print(f"  V2: {Path(v2_path).name}")

        # åŠ è½½å›¾ç‰‡
        img1 = cv2.imread(v1_path)
        img2 = cv2.imread(v2_path)

        if img1 is None or img2 is None:
            return {
                "success": False,
                "error": "å›¾ç‰‡åŠ è½½å¤±è´¥"
            }

        # é˜¶æ®µ1: å›¾åƒé…å‡†
        print("\nğŸ“ é˜¶æ®µ1: å›¾åƒé…å‡†ä¸­...")
        try:
            aligned_img1, H = align_images(img1, img2)
            print("  âœ… é…å‡†æˆåŠŸ")
        except ValueError as e:
            print(f"  âš ï¸  é…å‡†å¤±è´¥: {e}")
            print("  â†’ ä½¿ç”¨ç›´æ¥å¯¹æ¯”æ¨¡å¼")
            aligned_img1 = img1

        # é˜¶æ®µ2: å·®å¼‚åŒºåŸŸæ£€æµ‹
        print("\nğŸ” é˜¶æ®µ2: å·®å¼‚æ£€æµ‹ä¸­...")
        regions = detect_differences(aligned_img1, img2)
        print(f"  âœ… æ£€æµ‹åˆ° {len(regions)} ä¸ªå·®å¼‚åŒºåŸŸ")

        if len(regions) == 0:
            return {
                "success": True,
                "differences": [],
                "message": "æœªæ£€æµ‹åˆ°å·®å¼‚"
            }

        # é˜¶æ®µ3: OCRæ–‡å­—è¯†åˆ«
        print("\nğŸ“ é˜¶æ®µ3: OCRè¯†åˆ«ä¸­...")
        text_changes = self._analyze_text_changes(
            aligned_img1, img2, regions
        )
        print(f"  âœ… è¯†åˆ«åˆ° {len(text_changes)} å¤„æ–‡å­—å˜åŒ–")

        # é˜¶æ®µ4: AIè¯­ä¹‰åˆ†æ
        print("\nğŸ¤– é˜¶æ®µ4: AIè¯­ä¹‰åˆ†æä¸­...")
        differences = self._semantic_analysis(
            aligned_img1, img2, regions, text_changes
        )
        print(f"  âœ… åˆ†æå®Œæˆï¼Œå…± {len(differences)} å¤„å·®å¼‚")

        return {
            "success": True,
            "differences": differences,
            "method": "full" if self.vision.available else "basic"
        }

    def _analyze_text_changes(
        self,
        img1: np.ndarray,
        img2: np.ndarray,
        regions: List[Dict]
    ) -> List[Dict]:
        """ä½¿ç”¨OCRåˆ†ææ–‡å­—å˜åŒ–"""
        text_changes = []

        for i, region in enumerate(regions):
            # è£å‰ªåŒºåŸŸ
            x, y, w, h = region['x'], region['y'], region['width'], region['height']
            crop1 = img1[y:y+h, x:x+w]
            crop2 = img2[y:y+h, x:x+w]

            # ä¿å­˜ä¸´æ—¶æ–‡ä»¶
            temp1 = f"/tmp/crop1_{i}.png"
            temp2 = f"/tmp/crop2_{i}.png"
            cv2.imwrite(temp1, crop1)
            cv2.imwrite(temp2, crop2)

            # OCRè¯†åˆ«
            result1 = self.ocr.recognize(temp1)
            result2 = self.ocr.recognize(temp2)

            text1 = ' '.join([r['text'] for r in result1])
            text2 = ' '.join([r['text'] for r in result2])

            if text1 != text2:
                text_changes.append({
                    "region_index": i,
                    "old_text": text1,
                    "new_text": text2,
                    "location": region
                })

        return text_changes

    def _semantic_analysis(
        self,
        img1: np.ndarray,
        img2: np.ndarray,
        regions: List[Dict],
        text_changes: List[Dict]
    ) -> List[Dict]:
        """ä½¿ç”¨Qwen-VLè¿›è¡Œè¯­ä¹‰åˆ†æ"""
        differences = []

        for i, region in enumerate(regions):
            # æŸ¥æ‰¾å¯¹åº”çš„æ–‡å­—å˜åŒ–
            text_change = next(
                (tc for tc in text_changes if tc['region_index'] == i),
                None
            )

            # è£å‰ªä¸Šä¸‹æ–‡åŒºåŸŸ
            context1 = crop_with_context(img1, region, padding=0.2)
            context2 = crop_with_context(img2, region, padding=0.2)

            # ä¿å­˜ä¸´æ—¶æ–‡ä»¶
            temp_ctx1 = f"/tmp/ctx1_{i}.png"
            temp_ctx2 = f"/tmp/ctx2_{i}.png"
            cv2.imwrite(temp_ctx1, context1)
            cv2.imwrite(temp_ctx2, context2)

            # æ„å»ºæç¤ºè¯
            prompt = self._build_prompt(text_change)

            # è°ƒç”¨Qwen-VL
            if self.vision.available:
                result = self.vision.analyze([temp_ctx1, temp_ctx2], prompt)

                if result.get('error'):
                    # é™çº§ä¸ºåŸºç¡€æ¨¡å¼
                    result = self._basic_classification(text_change)
            else:
                result = self._basic_classification(text_change)

            differences.append({
                "id": i + 1,
                "location": region,
                "category": result.get('category', 'æœªçŸ¥å˜æ›´'),
                "description": result.get('description', 'æ£€æµ‹åˆ°å·®å¼‚'),
                "detail": result.get('detail', ''),
                "severity": result.get('severity', 'low')
            })

        return differences

    def _build_prompt(self, text_change: Dict | None) -> str:
        """æ„å»ºQwen-VLæç¤ºè¯"""
        ocr_info = ""
        if text_change:
            ocr_info = f"\nOCRè¯†åˆ«çš„æ–‡å­—å˜åŒ–ï¼š\næ—§ç‰ˆæœ¬: {text_change['old_text']}\næ–°ç‰ˆæœ¬: {text_change['new_text']}"

        return f"""ä½ æ˜¯å·¥ç¨‹å›¾çº¸åˆ†æä¸“å®¶ã€‚è¯·å¯¹æ¯”è¿™ä¸¤å¼ å·¥ç¨‹å›¾çº¸çš„å±€éƒ¨åŒºåŸŸã€‚

å·¦å›¾ï¼šæ—§ç‰ˆæœ¬ V1.0
å³å›¾ï¼šæ–°ç‰ˆæœ¬ V2.0

çº¢æ¡†æ ‡æ³¨åŒºåŸŸå‘ç”Ÿäº†å˜åŒ–ï¼Œè¯·åˆ†æï¼š
1. è¿™æ˜¯ä»€ä¹ˆç±»å‹çš„å˜æ›´ï¼Ÿï¼ˆå°ºå¯¸å˜æ›´/æ ‡æ³¨ä¿®æ”¹/å½¢çŠ¶å˜åŒ–/æ–°å¢å…ƒç´ /åˆ é™¤å…ƒç´ ï¼‰
2. å…·ä½“æ”¹äº†ä»€ä¹ˆï¼Ÿè¯·ç”¨å·¥ç¨‹æœ¯è¯­ç²¾ç¡®æè¿°
3. è¿™ä¸ªå˜æ›´çš„å½±å“ç¨‹åº¦ï¼Ÿï¼ˆlow=ç»†å¾®/medium=ä¸­ç­‰/high=é‡å¤§ï¼‰
{ocr_info}

è¯·ç”¨JSONæ ¼å¼å›ç­”ï¼ˆåªè¾“å‡ºJSONï¼‰ï¼š
{{
  "category": "å°ºå¯¸å˜æ›´",
  "description": "èºæ “å­”ç›´å¾„ä»M6æ”¹ä¸ºM8",
  "detail": "å­”å¾„å¢å¤§33.3%ï¼Œéœ€æ›´æ¢å¯¹åº”è§„æ ¼èºæ “",
  "severity": "medium"
}}
"""

    def _basic_classification(self, text_change: Dict | None) -> Dict:
        """åŸºç¡€åˆ†ç±»ï¼ˆæ— AIæ—¶çš„é™çº§æ–¹æ¡ˆï¼‰"""
        if not text_change:
            return {
                "category": "è§†è§‰å·®å¼‚",
                "description": "æ£€æµ‹åˆ°åŒºåŸŸå˜åŒ–",
                "detail": "å»ºè®®äººå·¥å®¡æ ¸",
                "severity": "low"
            }

        old = text_change['old_text']
        new = text_change['new_text']

        # ç®€å•è§„åˆ™åˆ¤æ–­
        if any(c in old or c in new for c in ['M', 'DN', 'Ã˜', 'Â°']):
            return {
                "category": "å°ºå¯¸å˜æ›´",
                "description": f"æ–‡å­—ä» '{old}' æ”¹ä¸º '{new}'",
                "detail": "å¯èƒ½æ¶‰åŠå°ºå¯¸å˜åŒ–",
                "severity": "medium"
            }
        else:
            return {
                "category": "æ ‡æ³¨ä¿®æ”¹",
                "description": f"æ–‡å­—ä» '{old}' æ”¹ä¸º '{new}'",
                "detail": "",
                "severity": "low"
            }
```

**Step 2: åˆ›å»ºæŠ¥å‘Šç”Ÿæˆå™¨**

Create: `services/document-recognition/drawing_comparison/report_generator.py`

```python
#!/usr/bin/env python3
"""
ç”Ÿæˆæ ‡æ³¨å›¾å’ŒæŠ¥å‘Š
"""
import cv2
import numpy as np
from typing import List, Dict

class ReportGenerator:
    # é¢œè‰²å®šä¹‰ï¼ˆBGRæ ¼å¼ï¼‰
    COLORS = {
        'å°ºå¯¸å˜æ›´': (0, 0, 255),      # çº¢è‰²
        'æ ‡æ³¨ä¿®æ”¹': (0, 165, 255),    # æ©™è‰²
        'æ–°å¢å…ƒç´ ': (0, 255, 0),      # ç»¿è‰²
        'åˆ é™¤å…ƒç´ ': (255, 0, 0),      # è“è‰²
        'å½¢çŠ¶å˜åŒ–': (255, 0, 255),    # å“çº¢
        'è§†è§‰å·®å¼‚': (128, 128, 128),  # ç°è‰²
    }

    def generate_annotated_image(
        self,
        base_image_path: str,
        differences: List[Dict],
        output_path: str
    ) -> str:
        """
        ç”Ÿæˆæ ‡æ³¨å›¾

        Returns:
            output_path
        """
        img = cv2.imread(base_image_path)

        for diff in differences:
            loc = diff['location']
            category = diff.get('category', 'è§†è§‰å·®å¼‚')
            diff_id = diff['id']

            # è·å–é¢œè‰²
            color = self.COLORS.get(category, (128, 128, 128))

            # ç»˜åˆ¶åŠé€æ˜çŸ©å½¢
            overlay = img.copy()
            cv2.rectangle(
                overlay,
                (loc['x'], loc['y']),
                (loc['x'] + loc['width'], loc['y'] + loc['height']),
                color,
                -1  # å¡«å……
            )
            img = cv2.addWeighted(overlay, 0.2, img, 0.8, 0)

            # ç»˜åˆ¶è¾¹æ¡†
            cv2.rectangle(
                img,
                (loc['x'], loc['y']),
                (loc['x'] + loc['width'], loc['y'] + loc['height']),
                color,
                3
            )

            # ç»˜åˆ¶ç¼–å·
            label = f"#{diff_id}"
            label_size = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.8, 2)[0]

            # æ ‡ç­¾èƒŒæ™¯
            cv2.rectangle(
                img,
                (loc['x'], loc['y'] - 30),
                (loc['x'] + label_size[0] + 10, loc['y']),
                color,
                -1
            )

            # æ ‡ç­¾æ–‡å­—
            cv2.putText(
                img,
                label,
                (loc['x'] + 5, loc['y'] - 8),
                cv2.FONT_HERSHEY_SIMPLEX,
                0.8,
                (255, 255, 255),
                2
            )

        # ä¿å­˜
        cv2.imwrite(output_path, img)
        print(f"  âœ… æ ‡æ³¨å›¾å·²ä¿å­˜: {output_path}")

        return output_path

    def generate_summary_report(self, differences: List[Dict]) -> Dict:
        """ç”Ÿæˆæ‘˜è¦æŠ¥å‘Š"""
        if not differences:
            return {
                "total": 0,
                "by_category": {},
                "by_severity": {}
            }

        # æŒ‰ç±»åˆ«ç»Ÿè®¡
        by_category = {}
        for diff in differences:
            cat = diff.get('category', 'æœªçŸ¥')
            by_category[cat] = by_category.get(cat, 0) + 1

        # æŒ‰ä¸¥é‡ç¨‹åº¦ç»Ÿè®¡
        by_severity = {}
        for diff in differences:
            sev = diff.get('severity', 'low')
            by_severity[sev] = by_severity.get(sev, 0) + 1

        return {
            "total": len(differences),
            "by_category": by_category,
            "by_severity": by_severity
        }
```

**Step 3: åˆ›å»ºFlaskæœåŠ¡ç«¯ç‚¹**

Modify: `services/document-recognition/app.py`

åœ¨æ–‡ä»¶æœ«å°¾æ·»åŠ æ–°è·¯ç”±ï¼š

```python
from drawing_comparison.diff_analyzer import DrawingDiffAnalyzer
from drawing_comparison.report_generator import ReportGenerator

@app.route('/api/drawing-diff/analyze', methods=['POST'])
def analyze_drawing_diff():
    """å›¾çº¸æ¯”å¯¹åˆ†ææ¥å£"""
    try:
        data = request.json
        task_id = data.get('taskId')
        v1_path = data.get('v1Path')
        v2_path = data.get('v2Path')

        if not all([task_id, v1_path, v2_path]):
            return jsonify({
                'success': False,
                'error': 'ç¼ºå°‘å¿…è¦å‚æ•°'
            }), 400

        # TODO: å»ºç«‹WebSocketè¿æ¥æ¨é€è¿›åº¦

        # æ‰§è¡Œåˆ†æ
        analyzer = DrawingDiffAnalyzer()
        result = analyzer.analyze(v1_path, v2_path)

        if not result['success']:
            return jsonify(result), 500

        # ç”Ÿæˆæ ‡æ³¨å›¾
        generator = ReportGenerator()
        annotated_path = f"/tmp/annotated_{task_id}.png"
        generator.generate_annotated_image(
            v2_path,
            result['differences'],
            annotated_path
        )

        # ç”Ÿæˆæ‘˜è¦
        summary = generator.generate_summary_report(result['differences'])

        return jsonify({
            'success': True,
            'taskId': task_id,
            'differences': result['differences'],
            'annotatedImagePath': annotated_path,
            'summary': summary,
            'method': result.get('method', 'basic')
        })

    except Exception as e:
        print(f"âŒ åˆ†æå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500
```

**Step 4: æµ‹è¯•PythonæœåŠ¡**

```bash
# å¯åŠ¨æœåŠ¡
cd services/document-recognition
python3 app.py &

# æµ‹è¯•å¥åº·æ£€æŸ¥
curl http://localhost:8086/health

# åœæ­¢æœåŠ¡
pkill -f "python3 app.py"
```

**Step 5: æäº¤**

```bash
git add services/document-recognition/drawing_comparison/ services/document-recognition/app.py
git commit -m "feat(ai): add drawing comparison analysis service

é˜¶æ®µ1: OpenCVé…å‡†+å·®å¼‚æ£€æµ‹
é˜¶æ®µ2: Deepseek-OCRæ–‡å­—è¯†åˆ«
é˜¶æ®µ3: Qwen-VLè¯­ä¹‰åˆ†æ
é˜¶æ®µ4: ç”Ÿæˆæ ‡æ³¨å›¾

æ”¯æŒAIé™çº§æ¨¡å¼

ğŸ¤– Generated with [Claude Code](https://claude.com/claude-code)

Co-Authored-By: Claude <noreply@anthropic.com>"
```

---

### Task 8: å‰ç«¯é¡µé¢å¼€å‘

**æ–‡ä»¶:**
- Create: `apps/web/src/pages/DrawingComparison.tsx`
- Modify: `apps/web/src/router/index.tsx`

**Step 1: åˆ›å»ºå‰ç«¯é¡µé¢**

```typescript
import React, { useState } from 'react';
import {
  Upload,
  Button,
  Card,
  Steps,
  message,
  Row,
  Col,
  List,
  Tag,
  Progress
} from 'antd';
import {
  InboxOutlined,
  CompareOutlined,
  DownloadOutlined
} from '@ant-design/icons';
import type { UploadFile } from 'antd/es/upload/interface';

const { Dragger } = Upload;

interface Difference {
  id: number;
  category: string;
  location: { x: number; y: number; width: number; height: number };
  description: string;
  detail: string;
  severity: 'low' | 'medium' | 'high';
}

const DrawingComparison: React.FC = () => {
  const [v1File, setV1File] = useState<UploadFile | null>(null);
  const [v2File, setV2File] = useState<UploadFile | null>(null);
  const [comparing, setComparing] = useState(false);
  const [progress, setProgress] = useState(0);
  const [currentStep, setCurrentStep] = useState('');
  const [differences, setDifferences] = useState<Difference[]>([]);
  const [annotatedImageUrl, setAnnotatedImageUrl] = useState('');

  // æ–‡ä»¶ä¸Šä¼ é…ç½®
  const uploadProps = {
    maxCount: 1,
    accept: '.pdf,.png,.jpg,.jpeg',
    beforeUpload: (file: File) => {
      const isValidType = ['application/pdf', 'image/png', 'image/jpeg'].includes(file.type);
      if (!isValidType) {
        message.error('åªæ”¯æŒ PDFã€PNGã€JPG æ ¼å¼ï¼');
        return false;
      }
      const isValidSize = file.size / 1024 / 1024 < 50;
      if (!isValidSize) {
        message.error('æ–‡ä»¶å¿…é¡»å°äº 50MBï¼');
        return false;
      }
      return false; // é˜»æ­¢è‡ªåŠ¨ä¸Šä¼ 
    }
  };

  // å¼€å§‹æ¯”å¯¹
  const handleCompare = async () => {
    if (!v1File || !v2File) {
      message.warning('è¯·ä¸Šä¼ V1å’ŒV2ä¸¤ä¸ªæ–‡ä»¶');
      return;
    }

    setComparing(true);
    setProgress(0);
    setCurrentStep('ä¸Šä¼ æ–‡ä»¶ä¸­...');

    try {
      // åˆ›å»ºFormData
      const formData = new FormData();
      formData.append('v1File', v1File as any);
      formData.append('v2File', v2File as any);

      // è°ƒç”¨åç«¯API
      const response = await fetch('/api/drawing-comparison/compare', {
        method: 'POST',
        headers: {
          'Authorization': `Bearer ${localStorage.getItem('token')}`
        },
        body: formData
      });

      const result = await response.json();

      if (result.success) {
        const { taskId } = result.data;

        // TODO: å»ºç«‹WebSocketè¿æ¥ç›‘å¬è¿›åº¦
        // ä¸´æ—¶ä½¿ç”¨è½®è¯¢æ¨¡æ‹Ÿ
        pollTaskStatus(taskId);
      } else {
        throw new Error(result.message);
      }
    } catch (error: any) {
      message.error(`æ¯”å¯¹å¤±è´¥: ${error.message}`);
      setComparing(false);
    }
  };

  // è½®è¯¢ä»»åŠ¡çŠ¶æ€ï¼ˆä¸´æ—¶æ–¹æ¡ˆï¼Œåç»­æ”¹ä¸ºWebSocketï¼‰
  const pollTaskStatus = (taskId: string) => {
    const timer = setInterval(async () => {
      try {
        const response = await fetch(`/api/drawing-comparison/status/${taskId}`, {
          headers: {
            'Authorization': `Bearer ${localStorage.getItem('token')}`
          }
        });
        const result = await response.json();

        if (result.success) {
          const { status, progress: p, currentStep: step } = result.data;
          setProgress(p);
          setCurrentStep(step || '');

          if (status === 'completed') {
            clearInterval(timer);
            fetchResult(taskId);
          } else if (status === 'failed') {
            clearInterval(timer);
            message.error(result.data.message || 'æ¯”å¯¹å¤±è´¥');
            setComparing(false);
          }
        }
      } catch (error) {
        console.error('è·å–çŠ¶æ€å¤±è´¥:', error);
      }
    }, 2000);
  };

  // è·å–ç»“æœ
  const fetchResult = async (taskId: string) => {
    try {
      const response = await fetch(`/api/drawing-comparison/result/${taskId}`, {
        headers: {
          'Authorization': `Bearer ${localStorage.getItem('token')}`
        }
      });
      const result = await response.json();

      if (result.success) {
        setDifferences(result.data.differences);
        setAnnotatedImageUrl(result.data.annotatedImageUrl);
        message.success(`æ¯”å¯¹å®Œæˆï¼å‘ç° ${result.data.summary.totalDifferences} å¤„å·®å¼‚`);
      }
    } catch (error: any) {
      message.error(`è·å–ç»“æœå¤±è´¥: ${error.message}`);
    } finally {
      setComparing(false);
    }
  };

  // ä¸¥é‡ç¨‹åº¦é¢œè‰²
  const getSeverityColor = (severity: string) => {
    const colors = {
      low: 'green',
      medium: 'orange',
      high: 'red'
    };
    return colors[severity as keyof typeof colors] || 'default';
  };

  return (
    <div style={{ padding: '24px' }}>
      <h2>å›¾çº¸æ¯”å¯¹</h2>

      <Row gutter={24}>
        {/* å·¦ä¾§ï¼šæ“ä½œåŒº */}
        <Col span={6}>
          <Card title="æ–‡ä»¶ä¸Šä¼ " size="small">
            <div style={{ marginBottom: 16 }}>
              <p style={{ marginBottom: 8 }}>æ—§ç‰ˆæœ¬ V1.0</p>
              <Dragger
                {...uploadProps}
                onChange={({ fileList }) => setV1File(fileList[0])}
                fileList={v1File ? [v1File] : []}
              >
                <p className="ant-upload-drag-icon">
                  <InboxOutlined />
                </p>
                <p className="ant-upload-text">ç‚¹å‡»æˆ–æ‹–æ‹½ä¸Šä¼ </p>
                <p className="ant-upload-hint">æ”¯æŒ PDF/PNG/JPGï¼Œæœ€å¤§ 50MB</p>
              </Dragger>
            </div>

            <div style={{ marginBottom: 16 }}>
              <p style={{ marginBottom: 8 }}>æ–°ç‰ˆæœ¬ V2.0</p>
              <Dragger
                {...uploadProps}
                onChange={({ fileList }) => setV2File(fileList[0])}
                fileList={v2File ? [v2File] : []}
              >
                <p className="ant-upload-drag-icon">
                  <InboxOutlined />
                </p>
                <p className="ant-upload-text">ç‚¹å‡»æˆ–æ‹–æ‹½ä¸Šä¼ </p>
                <p className="ant-upload-hint">æ”¯æŒ PDF/PNG/JPGï¼Œæœ€å¤§ 50MB</p>
              </Dragger>
            </div>

            <Button
              type="primary"
              icon={<CompareOutlined />}
              block
              size="large"
              onClick={handleCompare}
              disabled={!v1File || !v2File || comparing}
              loading={comparing}
            >
              å¼€å§‹æ¯”å¯¹
            </Button>
          </Card>

          {comparing && (
            <Card title="å¤„ç†è¿›åº¦" size="small" style={{ marginTop: 16 }}>
              <Progress percent={progress} status="active" />
              <p style={{ marginTop: 8, fontSize: 12, color: '#666' }}>
                {currentStep}
              </p>
              <Steps
                direction="vertical"
                size="small"
                current={Math.floor(progress / 25)}
                items={[
                  { title: 'ä¸Šä¼ æ–‡ä»¶' },
                  { title: 'å›¾åƒå¤„ç†' },
                  { title: 'AIåˆ†æä¸­' },
                  { title: 'ç”Ÿæˆç»“æœ' }
                ]}
              />
            </Card>
          )}
        </Col>

        {/* ä¸­é—´ï¼šç”»å¸ƒåŒº */}
        <Col span={12}>
          <Card
            title="æ¯”å¯¹ç»“æœ"
            size="small"
            extra={
              annotatedImageUrl && (
                <Button
                  icon={<DownloadOutlined />}
                  onClick={() => window.open(annotatedImageUrl)}
                >
                  å¯¼å‡ºæŠ¥å‘Š
                </Button>
              )
            }
          >
            {annotatedImageUrl ? (
              <img
                src={annotatedImageUrl}
                alt="æ ‡æ³¨å›¾"
                style={{ width: '100%', height: 'auto' }}
              />
            ) : (
              <div
                style={{
                  height: 400,
                  display: 'flex',
                  alignItems: 'center',
                  justifyContent: 'center',
                  background: '#f5f5f5'
                }}
              >
                <p style={{ color: '#999' }}>
                  ä¸Šä¼ æ–‡ä»¶å¹¶ç‚¹å‡»"å¼€å§‹æ¯”å¯¹"æŸ¥çœ‹ç»“æœ
                </p>
              </div>
            )}
          </Card>
        </Col>

        {/* å³ä¾§ï¼šå·®å¼‚åˆ—è¡¨ */}
        <Col span={6}>
          <Card
            title={`å‘ç° ${differences.length} å¤„å·®å¼‚`}
            size="small"
          >
            <List
              dataSource={differences}
              renderItem={(item) => (
                <List.Item>
                  <Card size="small" style={{ width: '100%' }}>
                    <div style={{ marginBottom: 8 }}>
                      <Tag color="blue">#{item.id}</Tag>
                      <Tag color={getSeverityColor(item.severity)}>
                        {item.category}
                      </Tag>
                    </div>
                    <p style={{ fontWeight: 'bold', marginBottom: 4 }}>
                      {item.description}
                    </p>
                    {item.detail && (
                      <p style={{ fontSize: 12, color: '#666' }}>
                        {item.detail}
                      </p>
                    )}
                  </Card>
                </List.Item>
              )}
            />
          </Card>
        </Col>
      </Row>
    </div>
  );
};

export default DrawingComparison;
```

**Step 2: æ³¨å†Œè·¯ç”±**

Modify: `apps/web/src/router/index.tsx`

æ·»åŠ å¯¼å…¥ï¼š
```typescript
const DrawingComparison = lazy(() => import('../pages/DrawingComparison'));
```

åœ¨routesæ•°ç»„ä¸­æ·»åŠ ï¼š
```typescript
{
  path: 'mechanical-design/drawing-comparison',
  element: <LazyWrapper Component={DrawingComparison} />
}
```

**Step 3: æäº¤**

```bash
git add apps/web/src/pages/DrawingComparison.tsx apps/web/src/router/index.tsx
git commit -m "feat(ui): add drawing comparison page

- ä¸‰æ å¸ƒå±€ï¼šä¸Šä¼ åŒº+ç”»å¸ƒåŒº+å·®å¼‚åˆ—è¡¨
- æ–‡ä»¶ä¸Šä¼ ï¼ˆæ‹–æ‹½æ”¯æŒï¼‰
- è¿›åº¦æ˜¾ç¤º
- å·®å¼‚å±•ç¤º

æš‚æ—¶ä½¿ç”¨è½®è¯¢ï¼Œåç»­æ”¹ä¸ºWebSocket

ğŸ¤– Generated with [Claude Code](https://claude.com/claude-code)

Co-Authored-By: Claude <noreply@anthropic.com>"
```

---

## æµ‹è¯•ä¸éƒ¨ç½²

### Task 9: ç«¯åˆ°ç«¯æµ‹è¯•

**Step 1: æ‰‹åŠ¨æµ‹è¯•**

```bash
# 1. å¯åŠ¨æ‰€æœ‰æœåŠ¡
docker-compose up -d postgres redis minio

# 2. å¯åŠ¨åç«¯
cd apps/api
PORT=3000 node src/app.js &

# 3. å¯åŠ¨PythonæœåŠ¡
cd services/document-recognition
python3 app.py &

# 4. å¯åŠ¨å‰ç«¯
cd apps/web
npm run dev

# 5. æµè§ˆå™¨è®¿é—®
open http://localhost:5178
# ç™»å½•åå¯¼èˆªåˆ°: æœºæ¢°è®¾è®¡ > å›¾çº¸æ¯”å¯¹

# 6. ä¸Šä¼ æµ‹è¯•æ–‡ä»¶å¹¶æ‰§è¡Œæ¯”å¯¹
```

**Step 2: éªŒè¯åŠŸèƒ½**

- âœ… æ–‡ä»¶ä¸Šä¼ æˆåŠŸ
- âœ… ä»»åŠ¡åˆ›å»ºæˆåŠŸ
- âœ… è¿›åº¦æ˜¾ç¤ºæ­£å¸¸
- âœ… AIåˆ†æå®Œæˆ
- âœ… æ ‡æ³¨å›¾ç”Ÿæˆ
- âœ… å·®å¼‚åˆ—è¡¨æ˜¾ç¤º

**Step 3: åœæ­¢æœåŠ¡**

```bash
pkill -f "node src/app.js"
pkill -f "python3 app.py"
```

---

### Task 10: æ–‡æ¡£æ›´æ–°

**æ–‡ä»¶:**
- Modify: `docs/CLAUDE.md`

**Step 1: æ›´æ–°APIæ–‡æ¡£**

åœ¨ `ğŸ“Š åç«¯APIæ¥å£æ–‡æ¡£` éƒ¨åˆ†æ·»åŠ ï¼š

```markdown
#### ğŸ–¼ï¸ å›¾çº¸æ¯”å¯¹ (/api/drawing-comparison) - 4ä¸ªæ¥å£
```
POST   /api/drawing-comparison/compare       åˆ›å»ºæ¯”å¯¹ä»»åŠ¡
GET    /api/drawing-comparison/status/:taskId è·å–ä»»åŠ¡çŠ¶æ€
GET    /api/drawing-comparison/result/:taskId è·å–æ¯”å¯¹ç»“æœ
GET    /api/drawing-comparison/tasks         è·å–ç”¨æˆ·ä»»åŠ¡åˆ—è¡¨
```
```

**Step 2: æ›´æ–°æœåŠ¡ç«¯å£**

åœ¨ `ğŸŒ æœåŠ¡ç«¯å£ä¸€è§ˆ` æ·»åŠ ï¼š

```markdown
| å›¾çº¸æ¯”å¯¹ | é›†æˆ | python3 app.py | âœ… è¿è¡Œä¸­ | `cd services/document-recognition && python3 app.py` |
```

**Step 3: æäº¤**

```bash
git add docs/CLAUDE.md
git commit -m "docs: add drawing comparison documentation

- æ–°å¢å›¾çº¸æ¯”å¯¹APIæ–‡æ¡£
- æ›´æ–°æœåŠ¡ç«¯å£åˆ—è¡¨
- æ·»åŠ ä½¿ç”¨è¯´æ˜

ğŸ¤– Generated with [Claude Code](https://claude.com/claude-code)

Co-Authored-By: Claude <noreply@anthropic.com>"
```

---

## æ€»ç»“

**å®Œæˆæƒ…å†µï¼š**
âœ… æ•°æ®åº“è¡¨åˆ›å»º
âœ… èœå•é…ç½®
âœ… åç«¯æœåŠ¡å±‚
âœ… åç«¯æ§åˆ¶å™¨
âœ… åç«¯è·¯ç”±
âœ… Python AIæœåŠ¡ï¼ˆé‡æ„+æ–°å¢ï¼‰
âœ… å‰ç«¯é¡µé¢
âœ… è·¯ç”±æ³¨å†Œ
âœ… ç«¯åˆ°ç«¯æµ‹è¯•
âœ… æ–‡æ¡£æ›´æ–°

**æŠ€æœ¯å€ºåŠ¡ï¼š**
âš ï¸ WebSocketå®æ—¶æ¨é€ï¼ˆå½“å‰ä½¿ç”¨è½®è¯¢ï¼‰
âš ï¸ Bullé˜Ÿåˆ—é›†æˆï¼ˆå½“å‰åŒæ­¥å¤„ç†ï¼‰
âš ï¸ å•å…ƒæµ‹è¯•è¦†ç›–
âš ï¸ é”™è¯¯å¤„ç†å®Œå–„

**ä¸‹ä¸€æ­¥ä¼˜åŒ–ï¼š**
1. å®ç°WebSocketè¿›åº¦æ¨é€
2. é›†æˆBullé˜Ÿåˆ—å¤„ç†å¹¶å‘
3. æ·»åŠ å•å…ƒæµ‹è¯•å’ŒE2Eæµ‹è¯•
4. ä¼˜åŒ–AIåˆ†æå‡†ç¡®ç‡
5. æ·»åŠ PDFæŠ¥å‘Šå¯¼å‡ºåŠŸèƒ½

---

**å®æ–½å®Œæ¯•ï¼æ€»æäº¤æ•°ï¼š10æ¬¡**
