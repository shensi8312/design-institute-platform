const knex = require('../../config/database')
const { SemanticChunk, SemanticDomain, SemanticType } = require('../../types/SemanticChunk')
const VectorStoreService = require('../utils/VectorStoreService')
const EmbeddingQueueService = require('./EmbeddingQueueService')
const CacheService = require('./CacheService')
const MinIOService = require('../storage/MinIOService')
const crypto = require('crypto')

/**
 * è¯­ä¹‰å±‚ç»Ÿä¸€æœåŠ¡
 *
 * åŠŸèƒ½:
 * 1. ç»Ÿä¸€çš„ Chunk ç´¢å¼•æ¥å£ (æ”¯æŒæ‰€æœ‰ domain/type)
 * 2. å‘é‡å­˜å‚¨æŠ½è±¡ (Milvus/Chroma/pgvector)
 * 3. å¼‚æ­¥ embedding é˜Ÿåˆ—
 * 4. å¢é‡ç´¢å¼• (åŸºäºå†…å®¹å“ˆå¸Œ)
 * 5. Redis ç¼“å­˜å±‚
 */
class SemanticLayerService {
  constructor() {
    this.vectorStore = new VectorStoreService()
    this.embeddingQueue = EmbeddingQueueService
    this.cache = CacheService
    this.initialized = false
  }

  /**
   * åˆå§‹åŒ–æœåŠ¡
   */
  async initialize() {
    if (this.initialized) return

    try {
      await this.vectorStore.initialize()
      await this.embeddingQueue.initialize()
      await this.cache.initialize()
      await MinIOService.initialize()
      this.initialized = true
      console.log('âœ… SemanticLayerService åˆå§‹åŒ–å®Œæˆ')
    } catch (error) {
      console.error('[SemanticLayer] åˆå§‹åŒ–å¤±è´¥:', error)
      throw error
    }
  }

  /**
   * ç´¢å¼• Chunks (æ”¯æŒå¢é‡æ›´æ–°)
   *
   * @param {string} domain - é¢†åŸŸæšä¸¾å€¼
   * @param {string} type - ç±»å‹æšä¸¾å€¼
   * @param {Array} chunks - å¾…ç´¢å¼•æ•°æ®
   * @param {Object} options - é€‰é¡¹
   */
  async indexChunks(domain, type, chunks, options = {}) {
    if (!this.initialized) await this.initialize()

    const {
      tenantId = null,
      projectId = null,
      immediate = false,
      incremental = true // é»˜è®¤å¯ç”¨å¢é‡ç´¢å¼•
    } = options

    try {
      console.log(`[SemanticLayer] ç´¢å¼• ${domain}/${type}: ${chunks.length} æ¡`)

      const semanticChunks = []
      const dbRecords = []
      const updateChunks = [] // éœ€è¦æ›´æ–°çš„
      const newChunks = []     // éœ€è¦æ–°å¢çš„

      for (const chunk of chunks) {
        const id = this._generateStableId(domain, type, chunk)
        const contentHash = this._generateContentHash(chunk.text)

        const semanticChunk = new SemanticChunk({
          id,
          domain,
          type,
          text: chunk.text,
          metadata: chunk.metadata || {},
          embedding: chunk.embedding || null,
          tenantId,
          projectId
        })

        // å¢é‡ç´¢å¼•æ£€æŸ¥
        if (incremental) {
          const existing = await knex('semantic_chunks')
            .where('id', id)
            .first()

          if (existing) {
            // æ£€æŸ¥å†…å®¹æ˜¯å¦å˜åŒ–
            if (existing.content_hash === contentHash) {
              console.log(`[SemanticLayer] è·³è¿‡æœªå˜åŒ–: ${id}`)
              continue
            }
            console.log(`[SemanticLayer] æ£€æµ‹åˆ°å˜åŒ–,æ›´æ–°: ${id}`)
            updateChunks.push(semanticChunk)
          } else {
            newChunks.push(semanticChunk)
          }
        } else {
          newChunks.push(semanticChunk)
        }

        semanticChunks.push(semanticChunk)

        dbRecords.push({
          id,
          domain,
          type,
          text: chunk.text,
          metadata: JSON.stringify(semanticChunk.metadata),
          content_hash: contentHash,
          tenant_id: tenantId,
          project_id: projectId,
          indexed_at: knex.fn.now()
        })
      }

      if (semanticChunks.length === 0) {
        console.log('[SemanticLayer] æ— éœ€æ›´æ–°,æ‰€æœ‰å†…å®¹æœªå˜åŒ–')
        return { success: true, indexed: 0, skipped: chunks.length }
      }

      console.log(`[SemanticLayer] å¢é‡ç»Ÿè®¡: æ–°å¢ ${newChunks.length}, æ›´æ–° ${updateChunks.length}`)

      // 1. å­˜å‚¨åˆ° semantic_chunks è¡¨
      await knex('semantic_chunks')
        .insert(dbRecords)
        .onConflict('id')
        .merge(['text', 'metadata', 'content_hash', 'indexed_at'])

      // 2. ç”Ÿæˆ embedding å¹¶ç´¢å¼•åˆ°å‘é‡åº“
      if (immediate) {
        console.log('[SemanticLayer] ç«‹å³ç”Ÿæˆ embedding å¹¶ç´¢å¼•')
        await this._embedAndIndex(semanticChunks)
      } else {
        console.log('[SemanticLayer] åŠ å…¥ embedding é˜Ÿåˆ—')
        await this.embeddingQueue.enqueue(semanticChunks.map(c => c.id))
      }

      // 3. æ¸…é™¤ç›¸å…³ç¼“å­˜
      await this._invalidateCache(domain, type, tenantId, projectId)

      return {
        success: true,
        indexed: semanticChunks.length,
        new: newChunks.length,
        updated: updateChunks.length,
        skipped: chunks.length - semanticChunks.length
      }

    } catch (error) {
      console.error('[SemanticLayer] ç´¢å¼•å¤±è´¥:', error)
      throw error
    }
  }

  /**
   * è¯­ä¹‰æœç´¢ (å¸¦ç¼“å­˜)
   */
  async search(query, filterObj = {}, topK = 10) {
    if (!this.initialized) await this.initialize()

    try {
      // å°è¯•ä»ç¼“å­˜è·å–
      const cacheParams = { query, filter: filterObj, topK }
      const cached = await this.cache.get('search', cacheParams)
      if (cached) {
        return cached
      }

      // ä»å‘é‡åº“æœç´¢
      console.log(`[SemanticLayer] æœç´¢: "${query}"`, filterObj)
      const results = await this.vectorStore.search(query, filterObj, topK)

      // ç¼“å­˜ç»“æœ (5åˆ†é’Ÿ)
      await this.cache.set('search', cacheParams, results, 300)

      return results

    } catch (error) {
      console.error('[SemanticLayer] æœç´¢å¤±è´¥:', error)
      throw error
    }
  }

  /**
   * æ‰¹é‡åˆ é™¤
   */
  async deleteChunks(ids) {
    if (!this.initialized) await this.initialize()

    try {
      // 1. ä»æ•°æ®åº“åˆ é™¤
      await knex('semantic_chunks').whereIn('id', ids).del()

      // 2. ä»å‘é‡åº“åˆ é™¤
      await this.vectorStore.delete(ids)

      // 3. æ¸…é™¤ç¼“å­˜
      await this.cache.invalidatePattern('search:*')

      console.log(`[SemanticLayer] å·²åˆ é™¤ ${ids.length} æ¡`)
      return { success: true, deleted: ids.length }

    } catch (error) {
      console.error('[SemanticLayer] åˆ é™¤å¤±è´¥:', error)
      throw error
    }
  }

  /**
   * ç”Ÿæˆç¨³å®š ID (åŸºäº domain/type/source_key çš„å“ˆå¸Œ)
   */
  _generateStableId(domain, type, chunk) {
    const srcKey =
      chunk.metadata?.kb_id ||
      chunk.metadata?.rule_code ||
      chunk.metadata?.template_section_id ||
      chunk.metadata?.graph_node_id ||
      chunk.metadata?.part_number ||
      chunk.text.substring(0, 50)

    const raw = `${domain}:${type}:${srcKey}`
    const hash = crypto.createHash('sha1').update(raw).digest('hex').substring(0, 16)
    return `${domain}:${type}:${hash}`
  }

  /**
   * ç”Ÿæˆå†…å®¹å“ˆå¸Œ (ç”¨äºå¢é‡ç´¢å¼•)
   */
  _generateContentHash(text) {
    return crypto.createHash('sha256').update(text).digest('hex').substring(0, 16)
  }

  /**
   * ç«‹å³ç”Ÿæˆ embedding å¹¶ç´¢å¼•
   */
  async _embedAndIndex(chunks) {
    const EmbeddingService = require('../rag/EmbeddingService')
    const embeddingService = new EmbeddingService()

    for (const chunk of chunks) {
      if (!chunk.embedding) {
        chunk.embedding = await embeddingService.generateEmbedding(chunk.text)
      }
    }

    await this.vectorStore.upsert(chunks)
    console.log(`[SemanticLayer] å·²ç´¢å¼• ${chunks.length} ä¸ªå‘é‡`)
  }

  /**
   * æ¸…é™¤ç¼“å­˜
   */
  async _invalidateCache(domain, type, tenantId, projectId) {
    const patterns = [
      'search:*',
      `search:${domain}:*`,
      tenantId ? `search:*:${tenantId}:*` : null
    ].filter(Boolean)

    for (const pattern of patterns) {
      await this.cache.invalidatePattern(pattern)
    }
  }

  /**
   * ä»çŸ¥è¯†åº“å¯¼å…¥
   */
  async importFromKnowledge(kbId, options = {}) {
    const docs = await knex('kb_documents')
      .where('kb_id', kbId)
      .select('*')

    const chunks = docs.map(doc => ({
      text: doc.content,
      metadata: {
        kb_id: kbId,
        doc_id: doc.id,
        doc_title: doc.title
      }
    }))

    return await this.indexChunks(
      SemanticDomain.KNOWLEDGE,
      SemanticType.CHUNK,
      chunks,
      options
    )
  }

  /**
   * ä»æ¨¡æ¿å¯¼å…¥
   */
  async importFromTemplates(templateId, options = {}) {
    const sections = await knex('template_sections')
      .where('template_id', templateId)
      .select('*')

    const chunks = sections.map(section => ({
      text: `${section.title}\n${section.content || ''}`,
      metadata: {
        template_id: templateId,
        template_section_id: section.id,
        section_code: section.section_code,
        section_title: section.title
      }
    }))

    return await this.indexChunks(
      SemanticDomain.SPEC,
      SemanticType.SECTION,
      chunks,
      options
    )
  }

  /**
   * ä»è§„åˆ™åº“å¯¼å…¥
   */
  async importFromRules(ruleType = null, options = {}) {
    let query = knex('design_rules').select('*')
    if (ruleType) {
      query = query.where('rule_type', ruleType)
    }

    const rules = await query

    const chunks = rules.map(rule => ({
      text: `${rule.rule_name}: ${rule.rule_description || ''}`,
      metadata: {
        rule_code: rule.rule_code,
        rule_name: rule.rule_name,
        rule_type: rule.rule_type,
        severity: rule.severity
      }
    }))

    return await this.indexChunks(
      SemanticDomain.RULE,
      SemanticType.RULE,
      chunks,
      options
    )
  }

  /**
   * ä»çŸ¥è¯†å›¾è°±å¯¼å…¥
   */
  async importFromGraph(graphId, options = {}) {
    const GraphService = require('../GraphService')
    const graphService = new GraphService()

    const nodes = await graphService.getAllNodes()

    const chunks = nodes.map(node => ({
      text: `${node.name}: ${node.description || ''}`,
      metadata: {
        graph_node_id: node.id,
        node_name: node.name,
        node_type: node.entity_type
      }
    }))

    return await this.indexChunks(
      SemanticDomain.GRAPH,
      SemanticType.NODE,
      chunks,
      options
    )
  }

  /**
   * ğŸ†• ä¸Šä¼ å¹¶å¤„ç†æ–‡æ¡£ (å®Œæ•´å·¥ä½œæµ: MinIO â†’ è§£æ â†’ ç´¢å¼•)
   */
  async uploadAndProcessDocument(fileBuffer, fileName, domain, type, options = {}) {
    if (!this.initialized) await this.initialize()

    try {
      const { tenantId = null, projectId = null, metadata = {} } = options

      console.log(`[SemanticLayer] å¤„ç†æ–‡æ¡£: ${fileName}`)

      // Step 1: ä¸Šä¼ åˆ° MinIO
      console.log('[SemanticLayer] 1/3 ä¸Šä¼ åˆ° MinIO...')
      const uploadResult = await MinIOService.uploadFile(fileBuffer, fileName, {
        domain,
        type,
        tenantId,
        projectId,
        ...metadata
      })

      // Step 2: è§£ææ–‡æ¡£ (ä½¿ç”¨ Docling æˆ–å…¶ä»–è§£æå™¨)
      console.log('[SemanticLayer] 2/3 è§£ææ–‡æ¡£...')
      const parseResult = await this._parseDocument(fileBuffer, fileName)

      // Step 3: ç´¢å¼•åˆ°è¯­ä¹‰å±‚
      console.log('[SemanticLayer] 3/3 ç´¢å¼•åˆ°è¯­ä¹‰å±‚...')
      const indexResult = await this.indexChunks(
        domain,
        type,
        parseResult.chunks,
        {
          tenantId,
          projectId,
          immediate: options.immediate || false,
          incremental: true
        }
      )

      return {
        success: true,
        file: {
          fileId: uploadResult.fileId,
          objectName: uploadResult.objectName,
          size: uploadResult.size,
          url: uploadResult.url
        },
        parse: {
          chunks: parseResult.chunks.length,
          parser: parseResult.parser
        },
        index: indexResult
      }

    } catch (error) {
      console.error('[SemanticLayer] æ–‡æ¡£å¤„ç†å¤±è´¥:', error)
      throw error
    }
  }

  /**
   * ğŸ†• ä» MinIO é‡æ–°å¤„ç†æ–‡æ¡£
   */
  async reprocessDocument(objectName, domain, type, options = {}) {
    if (!this.initialized) await this.initialize()

    try {
      console.log(`[SemanticLayer] é‡æ–°å¤„ç†æ–‡æ¡£: ${objectName}`)

      // ä» MinIO ä¸‹è½½
      const fileBuffer = await MinIOService.downloadFile(objectName)
      const fileInfo = await MinIOService.getFileInfo(objectName)

      // é‡æ–°è§£æå’Œç´¢å¼•
      const parseResult = await this._parseDocument(fileBuffer, objectName)

      const indexResult = await this.indexChunks(
        domain,
        type,
        parseResult.chunks,
        options
      )

      return {
        success: true,
        objectName,
        parse: {
          chunks: parseResult.chunks.length,
          parser: parseResult.parser
        },
        index: indexResult
      }

    } catch (error) {
      console.error('[SemanticLayer] é‡æ–°å¤„ç†å¤±è´¥:', error)
      throw error
    }
  }

  /**
   * è§£ææ–‡æ¡£ (å¯æ‰©å±•æ”¯æŒå¤šç§è§£æå™¨)
   */
  async _parseDocument(fileBuffer, fileName) {
    // è¿™é‡Œå¯ä»¥é›†æˆ Docling æˆ–å…¶ä»–è§£æå™¨
    // ç›®å‰ç®€å•ç¤ºä¾‹:å°†æ–‡æ¡£åˆ†æ®µ
    const text = fileBuffer.toString('utf-8').substring(0, 10000) // ç®€åŒ–å¤„ç†
    const chunks = this._simpleChunk(text)

    return {
      chunks,
      parser: 'simple'
    }
  }

  /**
   * ç®€å•åˆ†å— (ç¤ºä¾‹)
   */
  _simpleChunk(text) {
    const paragraphs = text.split(/\n\n+/).filter(p => p.trim().length > 0)
    return paragraphs.map((para, idx) => ({
      text: para.trim(),
      metadata: { chunk_index: idx }
    }))
  }

  /**
   * è·å–ç»Ÿè®¡ä¿¡æ¯
   */
  async getStats(domain = null, tenantId = null) {
    let query = knex('semantic_chunks')

    if (domain) {
      query = query.where('domain', domain)
    }

    if (tenantId) {
      query = query.where('tenant_id', tenantId)
    }

    const stats = await query
      .select('domain', 'type')
      .count('* as count')
      .groupBy('domain', 'type')

    const cacheStats = await this.cache.getStats()
    const vectorStats = await this.vectorStore.healthCheck()
    const minioStats = await MinIOService.healthCheck()

    return {
      chunks: stats,
      cache: cacheStats,
      vectorStore: vectorStats,
      storage: minioStats
    }
  }

  /**
   * å…³é—­æ‰€æœ‰è¿æ¥
   */
  async close() {
    await this.vectorStore.close()
    await this.cache.close()
    console.log('[SemanticLayer] æœåŠ¡å·²å…³é—­')
  }
}

module.exports = new SemanticLayerService()
