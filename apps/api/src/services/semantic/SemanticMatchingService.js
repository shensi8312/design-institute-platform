const db = require('../../config/database')

/**
 * Week 3: TF-IDFè¯­ä¹‰åŒ¹é…æœåŠ¡
 *
 * åŠŸèƒ½ï¼š
 * 1. é›¶ä»¶åç§°TF-IDFå‘é‡åŒ–ï¼ˆæ”¯æŒä¸­è‹±æ–‡ï¼‰
 * 2. ä½™å¼¦ç›¸ä¼¼åº¦è®¡ç®—
 * 3. ä»å†å²è£…é…æ•°æ®ä¸­æŸ¥æ‰¾è¯­ä¹‰ç›¸ä¼¼çš„é›¶ä»¶å¯¹
 */
class SemanticMatchingService {
  constructor() {
    this.cache = new Map()
    this.CACHE_TTL = 3600 * 1000 // 1å°æ—¶ç¼“å­˜
    this.SIMILARITY_THRESHOLD = 0.6 // ç›¸ä¼¼åº¦é˜ˆå€¼
  }

  /**
   * æ ‡å‡†åŒ–é›¶ä»¶åç§°ï¼ˆå»é™¤ç©ºæ ¼ã€ç»Ÿä¸€å¤§å°å†™ã€åˆ†è¯ï¼‰
   * @param {string} partName - é›¶ä»¶åç§°
   * @returns {Object} { normalized, tokens }
   */
  normalizeName(partName) {
    // 1. åŸºç¡€æ¸…ç†
    let normalized = partName
      .trim()
      .replace(/\s+/g, ' ')
      .toUpperCase()

    // 2. åˆ†è¯ï¼ˆæ”¯æŒä¸­è‹±æ–‡ï¼‰
    const tokens = []

    // æå–è‹±æ–‡å•è¯å’Œæ•°å­—
    const englishPattern = /[A-Z0-9]+/g
    const englishMatches = normalized.match(englishPattern) || []
    tokens.push(...englishMatches)

    // æå–ä¸­æ–‡å­—ç¬¦ï¼ˆ2-4å­—è¯ç»„ï¼‰
    const chinesePattern = /[\u4e00-\u9fa5]+/g
    const chineseMatches = normalized.match(chinesePattern) || []
    for (const match of chineseMatches) {
      // 2å­—è¯
      for (let i = 0; i < match.length - 1; i++) {
        tokens.push(match.substring(i, i + 2))
      }
      // 3å­—è¯
      for (let i = 0; i < match.length - 2; i++) {
        tokens.push(match.substring(i, i + 3))
      }
      // 4å­—è¯
      for (let i = 0; i < match.length - 3; i++) {
        tokens.push(match.substring(i, i + 4))
      }
    }

    // æå–è§„æ ¼ä¿¡æ¯
    const specs = this._extractSpecifications(normalized)
    tokens.push(...specs)

    return {
      normalized,
      tokens: [...new Set(tokens)] // å»é‡
    }
  }

  /**
   * æå–è§„æ ¼ä¿¡æ¯ï¼ˆèºçº¹ã€æ³•å…°ã€ç®¡é“ç­‰ï¼‰
   * @private
   */
  _extractSpecifications(name) {
    const specs = []

    // èºçº¹è§„æ ¼: M3, M8x1.25, 1/2", 3/4"
    const threadPattern = /M\d+(?:X\d+(?:\.\d+)?)?|\d+\/\d+"/gi
    const threads = name.match(threadPattern) || []
    specs.push(...threads.map(t => t.toUpperCase()))

    // å‹åŠ›ç­‰çº§: 150#, 300#, PN16, PN40, Class150
    const pressurePattern = /\d+#|PN\d+|CLASS\d+/gi
    const pressures = name.match(pressurePattern) || []
    specs.push(...pressures.map(p => p.toUpperCase()))

    // å…¬ç§°ç›´å¾„: DN50, DN100
    const dnPattern = /DN\d+/gi
    const dns = name.match(dnPattern) || []
    specs.push(...dns.map(d => d.toUpperCase()))

    // æè´¨: SS304, SS316, A105
    const materialPattern = /SS\d+|A\d+|[A-Z]{2}\d{3}/gi
    const materials = name.match(materialPattern) || []
    specs.push(...materials.map(m => m.toUpperCase()))

    return specs
  }

  /**
   * è®¡ç®—è¯é¢‘ï¼ˆTerm Frequencyï¼‰
   * @param {Array} tokens - è¯åˆ—è¡¨
   * @returns {Object} { term: frequency }
   */
  calculateTF(tokens) {
    const tf = {}
    const totalTokens = tokens.length

    for (const token of tokens) {
      tf[token] = (tf[token] || 0) + 1
    }

    // å½’ä¸€åŒ–
    for (const term in tf) {
      tf[term] = tf[term] / totalTokens
    }

    return tf
  }

  /**
   * ä»æ•°æ®åº“è®¡ç®—IDFï¼ˆInverse Document Frequencyï¼‰
   * @param {Array} terms - è¯åˆ—è¡¨
   * @returns {Object} { term: idf }
   */
  async calculateIDF(terms) {
    // 1. æŸ¥è¯¢æ‰€æœ‰å·²å‘é‡åŒ–çš„é›¶ä»¶
    const allVectors = await db('part_name_vectors')
      .select('part_name', 'term_frequencies')

    const totalDocs = allVectors.length || 1

    // 2. è®¡ç®—æ¯ä¸ªè¯çš„æ–‡æ¡£é¢‘ç‡
    const docFrequency = {}
    for (const term of terms) {
      let count = 0
      for (const vec of allVectors) {
        if (vec.term_frequencies && vec.term_frequencies[term]) {
          count++
        }
      }
      docFrequency[term] = count
    }

    // 3. è®¡ç®—IDF
    const idf = {}
    for (const term of terms) {
      const df = docFrequency[term] || 0
      idf[term] = Math.log((totalDocs + 1) / (df + 1)) + 1 // +1å¹³æ»‘
    }

    return idf
  }

  /**
   * è®¡ç®—TF-IDFå‘é‡
   * @param {string} partName - é›¶ä»¶åç§°
   * @returns {Object} TF-IDFå‘é‡
   */
  async calculateTFIDF(partName) {
    const { normalized, tokens } = this.normalizeName(partName)
    const tf = this.calculateTF(tokens)
    const idf = await this.calculateIDF(Object.keys(tf))

    const tfidf = {}
    for (const term in tf) {
      tfidf[term] = tf[term] * (idf[term] || 1)
    }

    return {
      normalized,
      tokens,
      tf,
      tfidf
    }
  }

  /**
   * ä½™å¼¦ç›¸ä¼¼åº¦è®¡ç®—
   * @param {Object} vectorA - TF-IDFå‘é‡A
   * @param {Object} vectorB - TF-IDFå‘é‡B
   * @returns {number} ç›¸ä¼¼åº¦ [0, 1]
   */
  cosineSimilarity(vectorA, vectorB) {
    const allTerms = new Set([
      ...Object.keys(vectorA),
      ...Object.keys(vectorB)
    ])

    let dotProduct = 0
    let magnitudeA = 0
    let magnitudeB = 0

    for (const term of allTerms) {
      const a = vectorA[term] || 0
      const b = vectorB[term] || 0

      dotProduct += a * b
      magnitudeA += a * a
      magnitudeB += b * b
    }

    if (magnitudeA === 0 || magnitudeB === 0) return 0

    return dotProduct / (Math.sqrt(magnitudeA) * Math.sqrt(magnitudeB))
  }

  /**
   * æ¨æ–­é›¶ä»¶ç±»åˆ«ï¼ˆåŸºäºåç§°å’Œè§„æ ¼ï¼‰
   * @param {string} partName - é›¶ä»¶åç§°
   * @returns {string} ç±»åˆ«
   */
  inferCategory(partName) {
    const name = partName.toUpperCase()

    // èºæ “ç±»
    if (/èºæ “|BOLT|HEX.*HEAD/i.test(name)) return 'bolt'
    if (/èºæ¯|NUT/i.test(name)) return 'nut'
    if (/å«åœˆ|WASHER/i.test(name)) return 'washer'
    if (/èºé’‰|SCREW/i.test(name)) return 'screw'

    // æ³•å…°ç±»
    if (/æ³•å…°|FLANGE/i.test(name)) return 'flange'
    if (/å«ç‰‡|GASKET/i.test(name)) return 'gasket'

    // é˜€é—¨ç±»
    if (/é˜€|VALVE/i.test(name)) return 'valve'
    if (/çƒé˜€|BALL.*VALVE/i.test(name)) return 'ball_valve'
    if (/é—¸é˜€|GATE.*VALVE/i.test(name)) return 'gate_valve'

    // ç®¡ä»¶ç±»
    if (/ç®¡|PIPE|TUBE/i.test(name)) return 'pipe'
    if (/ä¸‰é€š|TEE/i.test(name)) return 'tee'
    if (/å¼¯å¤´|ELBOW/i.test(name)) return 'elbow'
    if (/å¼‚å¾„|REDUCER/i.test(name)) return 'reducer'

    return 'unknown'
  }

  /**
   * å‘é‡åŒ–å•ä¸ªé›¶ä»¶å¹¶å­˜å‚¨åˆ°æ•°æ®åº“
   * @param {string} partName - é›¶ä»¶åç§°
   * @returns {Object} å‘é‡è®°å½•
   */
  async vectorizePart(partName) {
    // 1. æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
    const existing = await db('part_name_vectors')
      .where({ part_name: partName })
      .first()

    if (existing) {
      // æ›´æ–°å‡ºç°æ¬¡æ•°
      await db('part_name_vectors')
        .where({ part_name: partName })
        .increment('occurrence_count', 1)

      return existing
    }

    // 2. è®¡ç®—TF-IDFå‘é‡
    const result = await this.calculateTFIDF(partName)
    const category = this.inferCategory(partName)

    // 3. å­˜å‚¨åˆ°æ•°æ®åº“
    const [record] = await db('part_name_vectors')
      .insert({
        part_name: partName,
        part_name_normalized: result.normalized,
        tfidf_vector: result.tfidf,
        term_frequencies: result.tf,
        category,
        occurrence_count: 1
      })
      .returning('*')

    return record
  }

  /**
   * æ‰¹é‡å‘é‡åŒ–ï¼ˆä»assembly_datasetå¯¼å…¥ï¼‰
   * @param {string} datasetName - æ•°æ®é›†åç§°
   */
  async vectorizeDataset(datasetName) {
    // 1. æŸ¥è¯¢æ•°æ®é›†ä¸­çš„æ‰€æœ‰é›¶ä»¶åç§°
    const dataset = await db('assembly_dataset')
      .where({ dataset_name: datasetName })
      .select('part_a', 'part_b')

    const allPartNames = new Set()
    for (const row of dataset) {
      allPartNames.add(row.part_a)
      allPartNames.add(row.part_b)
    }

    console.log(`ğŸ“Š æ­£åœ¨å‘é‡åŒ– ${allPartNames.size} ä¸ªé›¶ä»¶...`)

    // 2. æ‰¹é‡å‘é‡åŒ–
    const results = []
    for (const partName of allPartNames) {
      const vector = await this.vectorizePart(partName)
      results.push(vector)
    }

    console.log(`âœ… å‘é‡åŒ–å®Œæˆ: ${results.length} ä¸ªé›¶ä»¶`)

    return results
  }

  /**
   * æŸ¥æ‰¾ç›¸ä¼¼é›¶ä»¶
   * @param {string} partName - æŸ¥è¯¢é›¶ä»¶åç§°
   * @param {number} topK - è¿”å›å‰Kä¸ªæœ€ç›¸ä¼¼çš„
   * @returns {Array} ç›¸ä¼¼é›¶ä»¶åˆ—è¡¨
   */
  async findSimilarParts(partName, topK = 5) {
    // 1. è®¡ç®—æŸ¥è¯¢é›¶ä»¶çš„TF-IDFå‘é‡
    const queryResult = await this.calculateTFIDF(partName)
    const queryVector = queryResult.tfidf

    // 2. æŸ¥è¯¢æ‰€æœ‰å·²å‘é‡åŒ–çš„é›¶ä»¶
    const allVectors = await db('part_name_vectors')
      .select('*')

    // 3. è®¡ç®—ç›¸ä¼¼åº¦
    const similarities = []
    for (const vec of allVectors) {
      if (vec.part_name === partName) continue // è·³è¿‡è‡ªå·±

      const similarity = this.cosineSimilarity(queryVector, vec.tfidf_vector)

      if (similarity >= this.SIMILARITY_THRESHOLD) {
        similarities.push({
          part_name: vec.part_name,
          category: vec.category,
          similarity: parseFloat(similarity.toFixed(4)),
          occurrence_count: vec.occurrence_count
        })
      }
    }

    // 4. æŒ‰ç›¸ä¼¼åº¦é™åºæ’åºï¼Œè¿”å›topK
    return similarities
      .sort((a, b) => b.similarity - a.similarity)
      .slice(0, topK)
  }

  /**
   * ä»å†å²æ•°æ®æ¨èè£…é…çº¦æŸ
   * @param {string} partA - é›¶ä»¶Aåç§°
   * @param {string} partB - é›¶ä»¶Båç§°
   * @returns {Array} æ¨èçš„çº¦æŸç±»å‹
   */
  async recommendConstraints(partA, partB) {
    // 1. æŸ¥æ‰¾ä¸partAç›¸ä¼¼çš„é›¶ä»¶
    const similarToA = await this.findSimilarParts(partA, 3)

    // 2. æŸ¥æ‰¾ä¸partBç›¸ä¼¼çš„é›¶ä»¶
    const similarToB = await this.findSimilarParts(partB, 3)

    // 3. æŸ¥è¯¢å†å²è£…é…æ•°æ®
    const constraints = []

    for (const simA of similarToA) {
      for (const simB of similarToB) {
        const history = await db('assembly_dataset')
          .where(function() {
            this.where({ part_a: simA.part_name, part_b: simB.part_name })
              .orWhere({ part_a: simB.part_name, part_b: simA.part_name })
          })
          .select('constraint_type', 'parameters', 'confidence')

        for (const record of history) {
          // è®¡ç®—ç»¼åˆç½®ä¿¡åº¦ = å†å²ç½®ä¿¡åº¦ Ã— è¯­ä¹‰ç›¸ä¼¼åº¦
          const semanticConfidence = (simA.similarity + simB.similarity) / 2
          const combinedConfidence = record.confidence * semanticConfidence

          constraints.push({
            constraint_type: record.constraint_type,
            parameters: record.parameters,
            confidence: parseFloat(combinedConfidence.toFixed(4)),
            reason: `åŸºäºç›¸ä¼¼é›¶ä»¶å¯¹ (${simA.part_name} â†” ${simB.part_name})`,
            similarity_a: simA.similarity,
            similarity_b: simB.similarity,
            historical_confidence: record.confidence
          })
        }
      }
    }

    // 4. å»é‡å¹¶æŒ‰ç½®ä¿¡åº¦æ’åº
    const uniqueConstraints = this._deduplicateConstraints(constraints)

    return uniqueConstraints
      .sort((a, b) => b.confidence - a.confidence)
      .slice(0, 5)
  }

  /**
   * å»é‡çº¦æŸï¼ˆç›¸åŒç±»å‹çš„å–æœ€é«˜ç½®ä¿¡åº¦ï¼‰
   * @private
   */
  _deduplicateConstraints(constraints) {
    const map = new Map()

    for (const c of constraints) {
      const key = c.constraint_type
      if (!map.has(key) || map.get(key).confidence < c.confidence) {
        map.set(key, c)
      }
    }

    return Array.from(map.values())
  }

  /**
   * è·å–è£…é…æ¨¡å¼ç»Ÿè®¡
   * @param {string} category - é›¶ä»¶ç±»åˆ«
   * @returns {Array} å¸¸è§æ¨¡å¼
   */
  async getAssemblyPatterns(category = null) {
    let query = db('assembly_patterns')
      .select('*')
      .orderBy('support_count', 'desc')
      .limit(10)

    if (category) {
      query = query.whereRaw("part_pattern->>'partA_type' = ? OR part_pattern->>'partB_type' = ?", [category, category])
    }

    const patterns = await query

    return patterns.map(p => ({
      pattern_name: p.pattern_name,
      description: p.description,
      constraint_type: p.constraint_type,
      support_count: p.support_count,
      confidence: parseFloat(p.confidence),
      is_validated: p.is_validated
    }))
  }
}

module.exports = SemanticMatchingService
