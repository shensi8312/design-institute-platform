#!/usr/bin/env python3
"""
å®Œæ•´æ–‡æ¡£ç¿»è¯‘è„šæœ¬ - è¶…çº§ä¼˜åŒ–ç‰ˆ
âœ… ä¿æŒWordæ ¼å¼ä¸å˜
âœ… æ‰¹é‡ç¿»è¯‘ï¼ˆ10ä¸ªæ®µè½/æ¬¡ï¼‰
âœ… ç§»é™¤é¡µçœ‰é¡µè„š
âœ… ç¿»è¯‘æ–‡ä»¶å¤¹åå’Œæ–‡ä»¶å
âœ… å¢é‡å¤„ç†ï¼ˆè·³è¿‡å·²å®Œæˆï¼‰
"""

import os
import sys
import json
import subprocess
from pathlib import Path
from typing import List, Dict
import requests

# å¼ºåˆ¶åˆ·æ–°è¾“å‡º
sys.stdout.reconfigure(line_buffering=True)
sys.stderr.reconfigure(line_buffering=True)

# ============ é…ç½® ============
SOURCE_DIR = "docs/specs/Full Length"
OUTPUT_DIR = "docs/specs_zh/Full Length"
TRANSLATION_MAP = "docs/specs/æ–‡ä»¶åç¿»è¯‘å¯¹ç…§è¡¨_å®Œæ•´ç‰ˆ.json"
VLLM_URL = os.getenv("VLLM_URL", "http://10.10.18.3:8000")
VLLM_MODEL = os.getenv("VLLM_MODEL", "/mnt/data/models/Qwen3-32B")
BATCH_SIZE = 10  # æ¯æ‰¹ç¿»è¯‘10ä¸ªæ®µè½

# ============ åŠ è½½ç¿»è¯‘å¯¹ç…§è¡¨ ============
def load_translation_map() -> Dict:
    """åŠ è½½æ–‡ä»¶åç¿»è¯‘å¯¹ç…§è¡¨"""
    try:
        with open(TRANSLATION_MAP, 'r', encoding='utf-8') as f:
            return json.load(f)
    except:
        return {"folders": {}, "files": {}}

TRANS_MAP = load_translation_map()

# ============ æ–‡ä»¶åç¿»è¯‘ ============
def translate_folder_name(folder_name: str) -> str:
    """ç¿»è¯‘æ–‡ä»¶å¤¹åï¼ˆä½¿ç”¨é¢„è®¾å¯¹ç…§è¡¨ï¼‰"""
    return TRANS_MAP.get("folders", {}).get(folder_name, folder_name)

def translate_file_name(file_name: str) -> str:
    """ç¿»è¯‘æ–‡ä»¶åï¼ˆä¿ç•™æ‰©å±•åï¼‰"""
    name_without_ext = file_name.replace('.DOC', '').replace('.docx', '')
    translated = TRANS_MAP.get("files", {}).get(file_name, name_without_ext)
    return f"{translated}.docx"

# ============ å†…å®¹è¿‡æ»¤ ============
def should_skip_paragraph(text: str) -> bool:
    """åˆ¤æ–­æ®µè½æ˜¯å¦åº”è¯¥è¢«è·³è¿‡ï¼ˆåˆ é™¤ï¼‰"""
    skip_keywords = [
        "Copyright",
        "The American Institute of Architects",
        "AIA",
        "Exclusively published and distributed by Deltek",
        "TIPS:",
        "To view non-printing Editor's Notes",
        "MasterWorks/Single-File Formatting",
        "MasterWorks/Supporting Information",
        "Content Requests:",
        "<Double click here to submit",
        "Double click here",
    ]

    # æ£€æŸ¥æ˜¯å¦åŒ…å«ä»»ä½•è·³è¿‡å…³é”®è¯
    for keyword in skip_keywords:
        if keyword in text:
            return True

    return False

# ============ ç¿»è¯‘API ============
def translate_single(text: str) -> str:
    """ç¿»è¯‘å•ä¸ªæ–‡æœ¬"""
    if not text.strip():
        return text

    prompt = f"""ç¿»è¯‘æˆä¸­æ–‡ï¼ˆä¿ç•™ä¸“ä¸šæœ¯è¯­å’Œæ ¼å¼ï¼‰ï¼š

{text}

ä¸­æ–‡ï¼š"""

    try:
        response = requests.post(
            f"{VLLM_URL}/v1/chat/completions",
            json={
                "model": VLLM_MODEL,
                "messages": [{"role": "user", "content": prompt}],
                "temperature": 0.3,
                "max_tokens": 1000
            },
            timeout=60
        )

        if response.status_code == 200:
            result = response.json()['choices'][0]['message']['content'].strip()

            # ç§»é™¤<think>æ ‡ç­¾å†…å®¹
            if '<think>' in result:
                if '</think>' in result:
                    result = result.split('</think>')[-1].strip()
                else:
                    # å¦‚æœæ²¡æœ‰é—­åˆæ ‡ç­¾ï¼Œè¿”å›åŸæ–‡
                    return text

            # æ¸…ç†å‰ç¼€
            if result.startswith("ä¸­æ–‡ï¼š") or result.startswith("ç¿»è¯‘ï¼š"):
                result = result.split("ï¼š", 1)[1].strip()

            # åªå–ç¬¬ä¸€è¡Œï¼ˆé¿å…å¤šä½™è§£é‡Šï¼‰
            result = result.split('\n')[0].strip()

            return result if result else text
        return text
    except:
        return text

def translate_batch(texts: List[str]) -> List[str]:
    """æ‰¹é‡ç¿»è¯‘å¤šä¸ªæ®µè½ï¼ˆ10ä¸ª/æ¬¡ï¼‰"""
    if not texts:
        return []

    # è¿‡æ»¤ç©ºæ®µè½
    non_empty_indices = [i for i, t in enumerate(texts) if t.strip()]
    non_empty_texts = [texts[i] for i in non_empty_indices]

    if not non_empty_texts:
        return texts

    # åˆå¹¶æ–‡æœ¬
    combined = "\n---SPLIT---\n".join(non_empty_texts)

    prompt = f"""ä½ æ˜¯å»ºç­‘å·¥ç¨‹è§„èŒƒç¿»è¯‘ä¸“å®¶ã€‚ç¿»è¯‘ä»¥ä¸‹{len(non_empty_texts)}ä¸ªæ®µè½ï¼ˆç”¨---SPLIT---åˆ†éš”ï¼‰ï¼š

{combined}

è¦æ±‚ï¼š
1. ä¿ç•™æ‰€æœ‰ä¸“ä¸šæœ¯è¯­ã€ç¼–å·ã€æ ¼å¼æ ‡è®°
2. ä¿æŒåŸæœ‰çš„æ¢è¡Œå’Œç¼©è¿›
3. ç”¨---SPLIT---åˆ†éš”ç¿»è¯‘ç»“æœ
4. ç›´æ¥è¾“å‡ºç¿»è¯‘ï¼Œä¸è¦åŠ "ç¿»è¯‘ï¼š"ç­‰å‰ç¼€

ä¸­æ–‡ç¿»è¯‘ï¼š"""

    try:
        response = requests.post(
            f"{VLLM_URL}/v1/chat/completions",
            headers={"Content-Type": "application/json"},
            json={
                "model": VLLM_MODEL,
                "messages": [{"role": "user", "content": prompt}],
                "temperature": 0.3,
                "max_tokens": len(combined) * 3
            },
            timeout=180
        )

        if response.status_code == 200:
            result = response.json()
            translated = result['choices'][0]['message']['content'].strip()

            # ç§»é™¤<think>æ ‡ç­¾å†…å®¹
            if '<think>' in translated:
                if '</think>' in translated:
                    translated = translated.split('</think>')[-1].strip()
                else:
                    # å¦‚æœæ²¡æœ‰é—­åˆï¼Œå›é€€åˆ°é€ä¸ªç¿»è¯‘
                    return [translate_single(t) for t in texts]

            # æ¸…ç†å¯èƒ½çš„å‰ç¼€
            if translated.startswith("ç¿»è¯‘ï¼š") or translated.startswith("ä¸­æ–‡ç¿»è¯‘ï¼š"):
                translated = translated.split("ï¼š", 1)[1].strip()

            # åˆ†å‰²ç»“æœ
            parts = translated.split("---SPLIT---")

            if len(parts) == len(non_empty_texts):
                # æ¢å¤åˆ°åŸå§‹ä½ç½®
                results = texts.copy()
                for i, idx in enumerate(non_empty_indices):
                    results[idx] = parts[i].strip()
                return results
            else:
                print(f"  âš ï¸  æ‰¹é‡åˆ†å‰²å¤±è´¥({len(parts)}!={len(non_empty_texts)})ï¼Œé€ä¸ªç¿»è¯‘...")
                # æ”¹ä¸ºé€ä¸ªç¿»è¯‘
                return [translate_single(t) for t in texts]
        else:
            print(f"  âš ï¸  APIé”™è¯¯: {response.status_code}")
            return texts
    except Exception as e:
        print(f"  âš ï¸  ç¿»è¯‘å¤±è´¥: {e}")
        return texts

# ============ Wordæ–‡æ¡£å¤„ç† ============
def process_word_document(input_file: Path, output_file: Path):
    """å¤„ç†Wordæ–‡æ¡£ï¼šç¿»è¯‘å†…å®¹ï¼Œç§»é™¤é¡µçœ‰é¡µè„šï¼Œä¿æŒæ ¼å¼"""
    print(f"\n{'='*70}")
    print(f"ğŸ“„ å¤„ç†: {input_file.name}")
    print(f"{'='*70}")

    # 1. DOCè½¬DOCX
    if input_file.suffix.upper() == '.DOC':
        temp_docx = Path(f"/tmp/{input_file.stem}_temp.docx")
        if sys.platform == 'darwin':
            print("ğŸ”„ DOCè½¬DOCX...")
            result = subprocess.run([
                'textutil', '-convert', 'docx',
                str(input_file), '-output', str(temp_docx)
            ], capture_output=True, text=True)

            if result.returncode != 0 or not temp_docx.exists():
                print(f"âŒ è½¬æ¢å¤±è´¥: {result.stderr}")
                return False
            working_file = temp_docx
        else:
            print("âŒ émacOSç³»ç»Ÿï¼Œè·³è¿‡")
            return False
    else:
        working_file = input_file

    # 2. åˆ›å»ºè¾“å‡ºç›®å½•
    output_file.parent.mkdir(parents=True, exist_ok=True)

    try:
        from docx import Document

        doc = Document(str(working_file))

        # 3. ç§»é™¤é¡µçœ‰é¡µè„š
        print("ğŸ—‘ï¸  ç§»é™¤é¡µçœ‰é¡µè„š...")
        for section in doc.sections:
            section.header.is_linked_to_previous = False
            section.footer.is_linked_to_previous = False
            for p in section.header.paragraphs:
                p.clear()
            for p in section.footer.paragraphs:
                p.clear()

        # 4. ç§»é™¤æ–‡æœ¬æ¡†å’Œå½¢çŠ¶
        print("ğŸ—‘ï¸  ç§»é™¤æ–‡æœ¬æ¡†å’Œå½¢çŠ¶...")
        try:
            # ç§»é™¤æ‰€æœ‰shapeï¼ˆåŒ…æ‹¬æ–‡æœ¬æ¡†ï¼‰
            for shape in doc.inline_shapes:
                shape._element.getparent().remove(shape._element)
        except:
            pass  # å¦‚æœæ²¡æœ‰shapeï¼Œå¿½ç•¥é”™è¯¯

        # 5. è¿‡æ»¤å¹¶æ”¶é›†éœ€è¦ç¿»è¯‘çš„æ®µè½
        paragraphs_data = []
        skipped_count = 0
        for p in doc.paragraphs:
            if p.text.strip():
                # æ£€æŸ¥æ˜¯å¦åº”è¯¥è·³è¿‡ï¼ˆåˆ é™¤ï¼‰
                if should_skip_paragraph(p.text):
                    p.clear()  # ç›´æ¥åˆ é™¤è¯¥æ®µè½
                    skipped_count += 1
                    continue

                # ä¿å­˜å®Œæ•´æ ¼å¼ï¼ˆåŒ…æ‹¬æ®µè½çº§åˆ«å’Œrunçº§åˆ«ï¼‰
                formats = []
                for run in p.runs:
                    formats.append({
                        'bold': run.bold,
                        'italic': run.italic,
                        'underline': run.underline,
                        'font_name': run.font.name,
                        'font_size': run.font.size,
                        'font_color': run.font.color.rgb if run.font.color and run.font.color.rgb else None
                    })

                # ä¿å­˜æ®µè½çº§åˆ«æ ¼å¼
                para_format = {
                    'style': p.style,  # æ®µè½æ ·å¼ï¼ˆPR1, PR2ç­‰ï¼‰
                    'alignment': p.alignment,  # å¯¹é½æ–¹å¼
                    'left_indent': p.paragraph_format.left_indent,  # å·¦ç¼©è¿›
                    'first_line_indent': p.paragraph_format.first_line_indent,  # é¦–è¡Œç¼©è¿›
                    'space_before': p.paragraph_format.space_before,  # æ®µå‰é—´è·
                    'space_after': p.paragraph_format.space_after,  # æ®µåé—´è·
                }

                paragraphs_data.append({
                    'paragraph': p,
                    'text': p.text,
                    'formats': formats,
                    'para_format': para_format
                })

        if skipped_count > 0:
            print(f"ğŸ—‘ï¸  å·²åˆ é™¤ {skipped_count} ä¸ªç‰ˆæƒ/æç¤ºæ®µè½")

        total = len(paragraphs_data)
        print(f"ğŸ“ æ‰¹é‡ç¿»è¯‘ {total} ä¸ªæ®µè½ï¼ˆ{BATCH_SIZE}ä¸ª/æ‰¹ï¼‰...")

        # 6. æ‰¹é‡ç¿»è¯‘æ®µè½
        success_count = 0
        for i in range(0, total, BATCH_SIZE):
            batch = paragraphs_data[i:i+BATCH_SIZE]
            batch_texts = [item['text'] for item in batch]

            progress = f"[{i+1}-{min(i+BATCH_SIZE, total)}/{total}]"
            print(f"{progress} ç¿»è¯‘ä¸­...", end='', flush=True)

            translated_texts = translate_batch(batch_texts)

            # åº”ç”¨ç¿»è¯‘ï¼ˆä¿æŒæ ¼å¼ï¼‰
            for j, (item, trans_text) in enumerate(zip(batch, translated_texts)):
                para = item['paragraph']
                para_fmt = item['para_format']

                # æœ€å®‰å…¨çš„æ–¹æ³•ï¼šåªä¿®æ”¹æ–‡æœ¬ï¼Œä¸åŠ¨æ®µè½ç»“æ„
                # è¿™æ ·å¯ä»¥ä¿ç•™æ®µè½æ ·å¼ï¼ˆPR1, ARTç­‰ï¼‰å’Œè‡ªåŠ¨ç¼–å·ï¼ˆA. B. C.ï¼‰

                if para.runs:
                    # å¦‚æœæœ‰å¤šä¸ªrunsï¼Œåˆå¹¶åˆ°ç¬¬ä¸€ä¸ªrun
                    if len(para.runs) > 1:
                        # ä¿å­˜ç¬¬ä¸€ä¸ªrunçš„æ ¼å¼
                        first_run = para.runs[0]

                        # åˆ é™¤å…¶ä»–runsï¼ˆä»åå¾€å‰åˆ ï¼‰
                        for i in range(len(para.runs) - 1, 0, -1):
                            p = para._element
                            p.remove(para.runs[i]._element)

                        # ä¿®æ”¹ç¬¬ä¸€ä¸ªrunçš„æ–‡æœ¬
                        first_run.text = trans_text
                    else:
                        # åªæœ‰ä¸€ä¸ªrunï¼Œç›´æ¥ä¿®æ”¹
                        para.runs[0].text = trans_text

                    # åº”ç”¨æ ¼å¼åˆ°ç¬¬ä¸€ä¸ªrun
                    if item['formats']:
                        run = para.runs[0]
                        fmt = item['formats'][0]
                        if fmt['bold'] is not None:
                            run.bold = fmt['bold']
                        if fmt['italic'] is not None:
                            run.italic = fmt['italic']
                        if fmt['underline'] is not None:
                            run.underline = fmt['underline']
                        if fmt['font_name']:
                            run.font.name = fmt['font_name']
                        if fmt['font_size']:
                            run.font.size = fmt['font_size']
                        if fmt['font_color']:
                            run.font.color.rgb = fmt['font_color']
                else:
                    # å¦‚æœæ²¡æœ‰runï¼Œåˆ›å»ºä¸€ä¸ª
                    run = para.add_run(trans_text)
                    if item['formats']:
                        fmt = item['formats'][0]
                        if fmt['bold'] is not None:
                            run.bold = fmt['bold']
                        if fmt['italic'] is not None:
                            run.italic = fmt['italic']
                        if fmt['underline'] is not None:
                            run.underline = fmt['underline']
                        if fmt['font_name']:
                            run.font.name = fmt['font_name']
                        if fmt['font_size']:
                            run.font.size = fmt['font_size']
                        if fmt['font_color']:
                            run.font.color.rgb = fmt['font_color']

            success_count += len(batch)
            print(f" âœ…")

        # 7. ç¿»è¯‘è¡¨æ ¼
        if doc.tables:
            print(f"\nğŸ“Š ç¿»è¯‘ {len(doc.tables)} ä¸ªè¡¨æ ¼...")
            for table_idx, table in enumerate(doc.tables, 1):
                cells_data = []
                for row in table.rows:
                    for cell in row.cells:
                        for para in cell.paragraphs:
                            if para.text.strip():
                                cells_data.append({
                                    'paragraph': para,
                                    'text': para.text,
                                    'bold': para.runs[0].bold if para.runs else None,
                                    'italic': para.runs[0].italic if para.runs else None
                                })

                if cells_data:
                    cell_texts = [c['text'] for c in cells_data]
                    print(f"  è¡¨æ ¼{table_idx}: {len(cell_texts)}ä¸ªå•å…ƒæ ¼...", end='', flush=True)
                    translated = translate_batch(cell_texts)

                    for cell_data, trans in zip(cells_data, translated):
                        para = cell_data['paragraph']

                        # å®‰å…¨ä¿®æ”¹ï¼šåªä¿®æ”¹æ–‡æœ¬ï¼Œä¸æ¸…ç©ºæ®µè½ï¼ˆä¿ç•™æ ·å¼å’Œç¼–å·ï¼‰
                        if para.runs:
                            # åˆ é™¤å¤šä½™çš„runsï¼ˆä»åå¾€å‰ï¼‰
                            if len(para.runs) > 1:
                                for i in range(len(para.runs) - 1, 0, -1):
                                    p = para._element
                                    p.remove(para.runs[i]._element)
                            # ä¿®æ”¹ç¬¬ä¸€ä¸ªrunçš„æ–‡æœ¬
                            para.runs[0].text = trans
                            para.runs[0].bold = cell_data['bold']
                            para.runs[0].italic = cell_data['italic']
                        else:
                            run = para.add_run(trans)
                            run.bold = cell_data['bold']
                            run.italic = cell_data['italic']

                    print(" âœ…")

        # 8. ä¿å­˜
        doc.save(str(output_file))
        try:
            rel_path = output_file.relative_to(Path.cwd())
            print(f"\nâœ… å®Œæˆï¼ä¿å­˜åˆ°: {rel_path}")
        except:
            print(f"\nâœ… å®Œæˆï¼ä¿å­˜åˆ°: {output_file}")

        # 9. æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        if input_file.suffix.upper() == '.DOC' and temp_docx.exists():
            temp_docx.unlink()

        return True

    except Exception as e:
        print(f"\nâŒ é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return False

# ============ æ‰¹é‡ç¿»è¯‘ ============
def batch_translate_all(max_files=None):
    """æ‰¹é‡ç¿»è¯‘æ‰€æœ‰æ–‡æ¡£ï¼ˆå¸¦æ–‡ä»¶åç¿»è¯‘ï¼‰"""
    source_path = Path(SOURCE_DIR)

    if not source_path.exists():
        print(f"âŒ æºç›®å½•ä¸å­˜åœ¨: {SOURCE_DIR}")
        return

    # è·å–æ‰€æœ‰å­æ–‡ä»¶å¤¹
    folders = sorted([d for d in source_path.iterdir() if d.is_dir() and not d.name.startswith('.')])

    print(f"\n{'='*70}")
    print(f"  ğŸš€ æ‰¹é‡æ–‡æ¡£ç¿»è¯‘ï¼ˆå®Œæ•´ç‰ˆï¼‰")
    print(f"{'='*70}")
    print(f"æºç›®å½•: {SOURCE_DIR}")
    print(f"è¾“å‡ºç›®å½•: {OUTPUT_DIR}")
    print(f"VLLM: {VLLM_URL}")
    print(f"æ¨¡å‹: {VLLM_MODEL}")
    print(f"æ‰¹é‡: {BATCH_SIZE} æ®µ/æ¬¡")
    print(f"æ–‡ä»¶å¤¹: {len(folders)} ä¸ª")
    print(f"ç¿»è¯‘è¡¨: {len(TRANS_MAP.get('folders', {}))} ä¸ªæ–‡ä»¶å¤¹å")
    print(f"{'='*70}\n")

    total_success = 0
    total_fail = 0
    total_skip = 0

    # éå†æ‰€æœ‰æ–‡ä»¶å¤¹
    for folder_idx, folder in enumerate(folders, 1):
        folder_name = folder.name
        translated_folder_name = translate_folder_name(folder_name)

        print(f"\n{'#'*70}")
        print(f"# æ–‡ä»¶å¤¹ [{folder_idx}/{len(folders)}]: {folder_name}")
        print(f"# ç¿»è¯‘ä¸º: {translated_folder_name}")
        print(f"{'#'*70}")

        # è·å–è¯¥æ–‡ä»¶å¤¹ä¸‹çš„æ‰€æœ‰æ–‡ä»¶
        files = sorted(list(folder.glob("*.DOC")) + list(folder.glob("*.docx")))
        # è¿‡æ»¤ä¸´æ—¶æ–‡ä»¶ï¼š~$ å¼€å¤´çš„å’Œ .~ å¼€å¤´çš„
        files = [f for f in files if not f.name.startswith('~$') and not f.name.startswith('.~')]

        if max_files:
            files = files[:max_files]

        # å¤„ç†æ¯ä¸ªæ–‡ä»¶
        for file_idx, input_file in enumerate(files, 1):
            file_name = input_file.name
            translated_file_name = translate_file_name(file_name)

            # è¾“å‡ºè·¯å¾„
            output_folder = Path(OUTPUT_DIR).parent / translated_folder_name
            output_file = output_folder / translated_file_name

            print(f"\n  [{file_idx}/{len(files)}] {file_name}")
            print(f"       â†’ {translated_file_name}")

            # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
            if output_file.exists():
                print(f"  â­ï¸  å·²å­˜åœ¨ï¼Œè·³è¿‡")
                total_skip += 1
                continue

            # ç¿»è¯‘æ–‡æ¡£
            try:
                success = process_word_document(input_file, output_file)
                if success:
                    total_success += 1
                else:
                    total_fail += 1
            except KeyboardInterrupt:
                print("\n\nâš ï¸  ç”¨æˆ·ä¸­æ–­")
                break
            except Exception as e:
                print(f"\n  âŒ å¤±è´¥: {e}")
                total_fail += 1

        if max_files:
            break  # æµ‹è¯•æ¨¡å¼åªå¤„ç†ä¸€ä¸ªæ–‡ä»¶å¤¹

    # ç»Ÿè®¡
    print(f"\n\n{'='*70}")
    print(f"  ğŸ“Š ç¿»è¯‘å®Œæˆ")
    print(f"{'='*70}")
    print(f"âœ… æˆåŠŸ: {total_success}")
    print(f"â­ï¸  è·³è¿‡: {total_skip}")
    print(f"âŒ å¤±è´¥: {total_fail}")
    print(f"è¾“å‡º: {OUTPUT_DIR}")
    print(f"{'='*70}\n")

# ============ ä¸»ç¨‹åº ============
if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description="å®Œæ•´æ–‡æ¡£ç¿»è¯‘ï¼ˆä¼˜åŒ–ç‰ˆï¼‰")
    parser.add_argument("--test", action="store_true", help="æµ‹è¯•æ¨¡å¼ï¼ˆ1ä¸ªæ–‡ä»¶ï¼‰")
    parser.add_argument("--small", action="store_true", help="å°æ‰¹é‡ï¼ˆ10ä¸ªæ–‡ä»¶ï¼‰")

    args = parser.parse_args()

    # æµ‹è¯•VLLMè¿æ¥
    print("ğŸ”Œ æµ‹è¯•VLLMè¿æ¥...")
    try:
        response = requests.get(f"{VLLM_URL}/v1/models", timeout=5)
        if response.status_code == 200:
            print(f"âœ… VLLMæœåŠ¡è¿æ¥æˆåŠŸ: {VLLM_URL}\n")
        else:
            print(f"âš ï¸  VLLMå“åº”å¼‚å¸¸: {response.status_code}")
    except Exception as e:
        print(f"âŒ VLLMè¿æ¥å¤±è´¥: {e}")
        print("è¯·æ£€æŸ¥VLLMæœåŠ¡æ˜¯å¦è¿è¡Œ")
        sys.exit(1)

    max_files = None
    if args.test:
        max_files = 1
    elif args.small:
        max_files = 10

    batch_translate_all(max_files)
