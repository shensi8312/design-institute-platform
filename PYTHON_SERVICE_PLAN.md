# Python ç®—æ³•æœåŠ¡è§„åˆ’

## ğŸ¯ æ¨èæ¶æ„ï¼šç‹¬ç«‹ Python å¾®æœåŠ¡

```
design-institute-platform/
â”œâ”€â”€ apps/
â”‚   â”œâ”€â”€ api/          # Node.js åç«¯ï¼ˆä¸»æœåŠ¡ï¼‰
â”‚   â””â”€â”€ web/          # React å‰ç«¯
â”‚
â””â”€â”€ services/
    â”œâ”€â”€ vector-service/   # å‘é‡æœç´¢æœåŠ¡
    â””â”€â”€ ml-service/       # Python ç®—æ³•æœåŠ¡ âœ¨ NEW
        â”œâ”€â”€ requirements.txt
        â”œâ”€â”€ Dockerfile
        â”œâ”€â”€ docker-compose.yml
        â”œâ”€â”€ main.py           # FastAPI å…¥å£
        â”œâ”€â”€ config/
        â”‚   â””â”€â”€ settings.py
        â”œâ”€â”€ algorithms/       # ç®—æ³•æ¨¡å—
        â”‚   â”œâ”€â”€ __init__.py
        â”‚   â”œâ”€â”€ ocr/          # OCR è¯†åˆ«
        â”‚   â”‚   â”œâ”€â”€ __init__.py
        â”‚   â”‚   â””â”€â”€ tesseract.py
        â”‚   â”œâ”€â”€ cv/           # è®¡ç®—æœºè§†è§‰
        â”‚   â”‚   â”œâ”€â”€ __init__.py
        â”‚   â”‚   â”œâ”€â”€ image_processor.py
        â”‚   â”‚   â””â”€â”€ feature_extractor.py
        â”‚   â”œâ”€â”€ cad/          # CAD å¤„ç†
        â”‚   â”‚   â”œâ”€â”€ __init__.py
        â”‚   â”‚   â”œâ”€â”€ step_parser.py
        â”‚   â”‚   â””â”€â”€ geometry.py
        â”‚   â””â”€â”€ ml/           # æœºå™¨å­¦ä¹ 
        â”‚       â”œâ”€â”€ __init__.py
        â”‚       â”œâ”€â”€ classifier.py
        â”‚       â””â”€â”€ embeddings.py
        â”œâ”€â”€ models/           # é¢„è®­ç»ƒæ¨¡å‹
        â”œâ”€â”€ tests/
        â””â”€â”€ utils/
```

---

## ğŸ”§ æŠ€æœ¯æ ˆå»ºè®®

### Python æœåŠ¡æ¡†æ¶
```python
# FastAPI (æ¨è) - é«˜æ€§èƒ½ã€è‡ªåŠ¨æ–‡æ¡£
from fastapi import FastAPI
app = FastAPI()

@app.post("/api/ocr")
async def ocr_image(file: UploadFile):
    # OCR å¤„ç†
    return {"text": "è¯†åˆ«ç»“æœ"}

@app.post("/api/classify")
async def classify_document(data: dict):
    # æ–‡æ¡£åˆ†ç±»
    return {"category": "æŠ€æœ¯å›¾çº¸"}
```

### æ ¸å¿ƒä¾èµ–
```txt
# requirements.txt
fastapi==0.104.1
uvicorn==0.24.0
numpy==1.24.3
opencv-python==4.8.1
pillow==10.1.0
pytesseract==0.3.10
torch==2.1.0           # å¦‚éœ€æ·±åº¦å­¦ä¹ 
transformers==4.35.2   # å¦‚éœ€ NLP
pythonOCC-core==7.7.0  # CAD å¤„ç†ï¼ˆä½ å·²æœ‰ï¼‰
pydantic==2.5.0
python-multipart==0.0.6
```

---

## ğŸ”— ä¸ Node.js åç«¯é€šä¿¡

### Node.js è°ƒç”¨ Python æœåŠ¡

```javascript
// apps/api/src/services/ml/client.js
const axios = require('axios');

class MLServiceClient {
  constructor() {
    this.baseURL = process.env.ML_SERVICE_URL || 'http://localhost:8001';
  }

  async classifyDocument(fileBuffer) {
    const formData = new FormData();
    formData.append('file', fileBuffer);

    const response = await axios.post(
      `${this.baseURL}/api/classify`,
      formData
    );
    return response.data;
  }

  async extractCADFeatures(stepFile) {
    const response = await axios.post(
      `${this.baseURL}/api/cad/extract`,
      { file_path: stepFile }
    );
    return response.data;
  }
}

module.exports = new MLServiceClient();
```

### åœ¨æ§åˆ¶å™¨ä¸­ä½¿ç”¨

```javascript
// apps/api/src/controllers/documentController.js
const mlService = require('../services/ml/client');

async function uploadDocument(req, res) {
  const file = req.file;

  // è°ƒç”¨ Python æœåŠ¡è¿›è¡Œåˆ†ç±»
  const classification = await mlService.classifyDocument(file.buffer);

  // ä¿å­˜åˆ°æ•°æ®åº“
  await Document.create({
    filename: file.originalname,
    category: classification.category,
    confidence: classification.confidence
  });

  res.json({ success: true, classification });
}
```

---

## ğŸ³ Docker é…ç½®

### Python æœåŠ¡ Dockerfile
```dockerfile
# services/ml-service/Dockerfile
FROM python:3.10-slim

WORKDIR /app

# å®‰è£…ç³»ç»Ÿä¾èµ–
RUN apt-get update && apt-get install -y \
    tesseract-ocr \
    tesseract-ocr-chi-sim \
    libgl1-mesa-glx \
    && rm -rf /var/lib/apt/lists/*

# å®‰è£… Python ä¾èµ–
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# å¤åˆ¶ä»£ç 
COPY . .

# å¯åŠ¨æœåŠ¡
CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8001"]
```

### æ·»åŠ åˆ° docker-compose.yml
```yaml
# infrastructure/docker/docker-compose.yml
services:
  # ... ç°æœ‰æœåŠ¡ ...

  ml-service:
    build: ../../services/ml-service
    ports:
      - "8001:8001"
    volumes:
      - ../../services/ml-service:/app
      - ml-models:/app/models
    environment:
      - MODEL_PATH=/app/models
      - LOG_LEVEL=info
    depends_on:
      - redis
      - minio

volumes:
  ml-models:
```

---

## ğŸ“ ç¤ºä¾‹ï¼šå®Œæ•´çš„ Python æœåŠ¡

### main.py
```python
from fastapi import FastAPI, UploadFile, File
from fastapi.middleware.cors import CORSMiddleware
import uvicorn

from algorithms.ocr.tesseract import TesseractOCR
from algorithms.cad.step_parser import StepParser
from algorithms.ml.classifier import DocumentClassifier

app = FastAPI(title="ML Service", version="1.0.0")

# CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_methods=["*"],
    allow_headers=["*"],
)

# åˆå§‹åŒ–ç®—æ³•
ocr_engine = TesseractOCR()
step_parser = StepParser()
classifier = DocumentClassifier()

@app.get("/health")
async def health():
    return {"status": "healthy"}

@app.post("/api/ocr")
async def ocr_document(file: UploadFile = File(...)):
    """OCR æ–‡å­—è¯†åˆ«"""
    content = await file.read()
    text = ocr_engine.extract_text(content)
    return {"text": text, "confidence": 0.95}

@app.post("/api/classify")
async def classify_document(file: UploadFile = File(...)):
    """æ–‡æ¡£åˆ†ç±»"""
    content = await file.read()
    result = classifier.predict(content)
    return {
        "category": result["category"],
        "confidence": result["confidence"],
        "tags": result["tags"]
    }

@app.post("/api/cad/parse")
async def parse_step_file(file: UploadFile = File(...)):
    """è§£æ STEP CAD æ–‡ä»¶"""
    content = await file.read()
    result = step_parser.parse(content)
    return {
        "parts": result["parts"],
        "assemblies": result["assemblies"],
        "metadata": result["metadata"]
    }

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8001)
```

---

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. åˆ›å»ºæœåŠ¡ç›®å½•
```bash
mkdir -p services/ml-service/{algorithms/{ocr,cv,cad,ml},models,tests}
cd services/ml-service
```

### 2. å®‰è£…ä¾èµ–
```bash
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate
pip install -r requirements.txt
```

### 3. å¯åŠ¨å¼€å‘æœåŠ¡å™¨
```bash
uvicorn main:app --reload --port 8001
```

### 4. æµ‹è¯• API
```bash
# è®¿é—®è‡ªåŠ¨æ–‡æ¡£
open http://localhost:8001/docs

# æµ‹è¯•å¥åº·æ£€æŸ¥
curl http://localhost:8001/health
```

---

## ğŸ”„ å¼€å‘å·¥ä½œæµ

### Python æœåŠ¡å¼€å‘
```bash
cd services/ml-service
source venv/bin/activate

# å¼€å‘æ–°ç®—æ³•
# 1. åœ¨ algorithms/ ä¸‹åˆ›å»ºæ–°æ¨¡å—
# 2. åœ¨ main.py ä¸­æ·»åŠ æ–°ç«¯ç‚¹
# 3. æµ‹è¯•
pytest tests/

# å¯åŠ¨
uvicorn main:app --reload
```

### Node.js è°ƒç”¨
```bash
cd apps/api

# ä½¿ç”¨ ML æœåŠ¡
node -e "
const client = require('./src/services/ml/client');
client.classifyDocument(buffer).then(console.log);
"
```

---

## ğŸ“Š æ€§èƒ½ä¼˜åŒ–å»ºè®®

### 1. æ¨¡å‹ç¼“å­˜
```python
from functools import lru_cache

@lru_cache(maxsize=1)
def load_model():
    # åªåŠ è½½ä¸€æ¬¡æ¨¡å‹
    return torch.load('models/classifier.pt')
```

### 2. æ‰¹å¤„ç†
```python
@app.post("/api/batch/classify")
async def batch_classify(files: List[UploadFile]):
    # æ‰¹é‡å¤„ç†æé«˜æ•ˆç‡
    results = []
    for file in files:
        result = classifier.predict(await file.read())
        results.append(result)
    return {"results": results}
```

### 3. å¼‚æ­¥å¤„ç†
```python
import asyncio

@app.post("/api/async/process")
async def async_process(file: UploadFile):
    # é•¿æ—¶é—´ä»»åŠ¡å¼‚æ­¥å¤„ç†
    task_id = str(uuid.uuid4())
    asyncio.create_task(process_heavy_task(task_id, file))
    return {"task_id": task_id, "status": "processing"}
```

---

## ğŸ” å®‰å…¨å»ºè®®

1. **API è®¤è¯**ï¼šä¸ Node.js å…±äº« JWT
2. **è¾“å…¥éªŒè¯**ï¼šä½¿ç”¨ Pydantic æ¨¡å‹
3. **æ–‡ä»¶å¤§å°é™åˆ¶**ï¼šé˜²æ­¢å¤§æ–‡ä»¶æ”»å‡»
4. **é€Ÿç‡é™åˆ¶**ï¼šä½¿ç”¨ slowapi

---

## ğŸ“ˆ ç›‘æ§å’Œæ—¥å¿—

```python
import logging
from prometheus_client import Counter, Histogram

# æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# æŒ‡æ ‡
request_count = Counter('ml_requests_total', 'Total requests')
request_duration = Histogram('ml_request_duration_seconds', 'Request duration')

@app.middleware("http")
async def monitor(request, call_next):
    request_count.inc()
    with request_duration.time():
        response = await call_next(request)
    return response
```

---

## âœ… æ€»ç»“

**æ¨èæ¶æ„**ï¼š
```
Node.js API (ä¸»æœåŠ¡ï¼Œç«¯å£ 3000)
    â†“ HTTP è°ƒç”¨
Python ML Service (ç®—æ³•æœåŠ¡ï¼Œç«¯å£ 8001)
    â†“ å¤„ç†
    - OCR è¯†åˆ«
    - å›¾åƒå¤„ç†
    - CAD è§£æ
    - æœºå™¨å­¦ä¹ 
```

**ä¼˜ç‚¹**ï¼š
- âœ… èŒè´£åˆ†ç¦»ï¼šNode.js å¤„ç†ä¸šåŠ¡é€»è¾‘ï¼ŒPython å¤„ç†ç®—æ³•
- âœ… ç‹¬ç«‹éƒ¨ç½²ï¼šå¯ä»¥å•ç‹¬æ‰©å±• Python æœåŠ¡
- âœ… æŠ€æœ¯é€‰å‹è‡ªç”±ï¼šPython ç”¨æœ€åˆé€‚çš„ ML åº“
- âœ… æ˜“äºç»´æŠ¤ï¼šå›¢é˜Ÿå¯ä»¥å¹¶è¡Œå¼€å‘

**ä¸‹ä¸€æ­¥**ï¼š
1. åˆ›å»º `services/ml-service/` ç›®å½•
2. å®ç°ç¬¬ä¸€ä¸ªç®—æ³•ï¼ˆå¦‚ OCRï¼‰
3. åœ¨ Node.js ä¸­é›†æˆè°ƒç”¨
4. æ·»åŠ åˆ° docker-compose.yml
