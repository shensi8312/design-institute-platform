#!/usr/bin/env python3
"""
æ›´å¥å£®çš„æ–‡æ¡£è¯†åˆ«é˜Ÿåˆ—æ¶ˆè´¹è€…
- æ›´å¥½çš„é”™è¯¯å¤„ç†
- è¯¦ç»†çš„æ—¥å¿—
- å¤±è´¥é‡è¯•æœºåˆ¶
"""

import redis
import json
import time
import requests
from datetime import datetime
import psycopg2
import traceback
import sys

# é…ç½®
REDIS_HOST = 'localhost'
REDIS_PORT = 6379
POSTGRES_CONFIG = {
    'host': 'localhost',
    'port': 5433,
    'database': 'design_platform',
    'user': 'postgres',
    'password': 'postgres'
}

# Redisè¿æ¥
redis_client = redis.Redis(host=REDIS_HOST, port=REDIS_PORT, decode_responses=True)

def get_db_connection():
    """è·å–æ•°æ®åº“è¿æ¥"""
    return psycopg2.connect(**POSTGRES_CONFIG)

def log(message, level="INFO"):
    """ç»Ÿä¸€çš„æ—¥å¿—è¾“å‡º"""
    timestamp = datetime.now().strftime('%H:%M:%S')
    prefix = {
        "INFO": "â„¹ï¸",
        "SUCCESS": "âœ…",
        "ERROR": "âŒ",
        "WARNING": "âš ï¸",
        "PROCESSING": "ğŸ”„"
    }.get(level, "ğŸ“")
    
    print(f"[{timestamp}] {prefix} {message}", flush=True)

def update_status(doc_id, field, status):
    """æ›´æ–°æ–‡æ¡£çŠ¶æ€"""
    try:
        conn = get_db_connection()
        cur = conn.cursor()
        cur.execute(f"""
            UPDATE knowledge_documents 
            SET {field} = %s, updated_at = NOW()
            WHERE id = %s
        """, (status, doc_id))
        conn.commit()
        cur.close()
        conn.close()
        log(f"çŠ¶æ€æ›´æ–°: {doc_id} - {field}={status}", "INFO")
        return True
    except Exception as e:
        log(f"çŠ¶æ€æ›´æ–°å¤±è´¥: {e}", "ERROR")
        return False

def process_vectorization(doc_id, text, kb_id, filename):
    """å¤„ç†å‘é‡åŒ–"""
    log(f"å¼€å§‹å‘é‡åŒ–: {doc_id}", "PROCESSING")
    
    # å…ˆæ›´æ–°ä¸ºprocessing
    update_status(doc_id, 'vector_status', 'processing')
    
    try:
        response = requests.post(
            'http://localhost:8085/api/vectorize',
            json={
                'doc_id': doc_id,
                'content': text,
                'kb_id': kb_id,
                'namespace': f'kb_{kb_id}',
                'chunk_size': 500,
                'chunk_overlap': 50,
                'metadata': {
                    'filename': filename,
                    'source': 'robust_consumer',
                    'timestamp': datetime.now().isoformat()
                }
            },
            timeout=60
        )
        
        if response.status_code == 200:
            result = response.json()
            chunks = result.get('chunks', 0)
            log(f"å‘é‡åŒ–æˆåŠŸ: {chunks} ä¸ªå—", "SUCCESS")
            update_status(doc_id, 'vector_status', 'completed')
            return True
        else:
            log(f"å‘é‡åŒ–å¤±è´¥: HTTP {response.status_code}", "ERROR")
            log(f"å“åº”: {response.text[:200]}", "ERROR")
            update_status(doc_id, 'vector_status', 'failed')
            return False
            
    except requests.exceptions.Timeout:
        log(f"å‘é‡åŒ–è¶…æ—¶", "ERROR")
        update_status(doc_id, 'vector_status', 'failed')
        return False
    except Exception as e:
        log(f"å‘é‡åŒ–å¼‚å¸¸: {e}", "ERROR")
        update_status(doc_id, 'vector_status', 'failed')
        return False

def process_graph_extraction(doc_id, text, filename):
    """å¤„ç†å›¾è°±æå–"""
    log(f"å¼€å§‹å›¾è°±æå–: {doc_id}", "PROCESSING")
    
    # å…ˆæ›´æ–°ä¸ºprocessing
    update_status(doc_id, 'graph_status', 'processing')
    
    try:
        # é™åˆ¶æ–‡æœ¬é•¿åº¦
        text_for_graph = text[:10000] if len(text) > 10000 else text
        
        response = requests.post(
            'http://localhost:8081/api/extract',
            json={
                'doc_id': doc_id,
                'text': text_for_graph,
                'metadata': {
                    'filename': filename,
                    'source': 'robust_consumer'
                }
            },
            timeout=60
        )
        
        if response.status_code == 200:
            result = response.json()
            entities = len(result.get('entities', []))
            relations = len(result.get('relations', []))
            log(f"å›¾è°±æå–æˆåŠŸ: {entities} å®ä½“, {relations} å…³ç³»", "SUCCESS")
            update_status(doc_id, 'graph_status', 'completed')
            return True
        else:
            log(f"å›¾è°±æå–å¤±è´¥: HTTP {response.status_code}", "ERROR")
            log(f"å“åº”: {response.text[:200]}", "ERROR")
            update_status(doc_id, 'graph_status', 'failed')
            return False
            
    except requests.exceptions.Timeout:
        log(f"å›¾è°±æå–è¶…æ—¶", "ERROR")
        update_status(doc_id, 'graph_status', 'failed')
        return False
    except Exception as e:
        log(f"å›¾è°±æå–å¼‚å¸¸: {e}", "ERROR")
        update_status(doc_id, 'graph_status', 'failed')
        return False

def process_recognition_result(recognition_data):
    """å¤„ç†è¯†åˆ«ç»“æœ"""
    doc_id = recognition_data.get('doc_id')
    kb_id = recognition_data.get('kb_id')
    filename = recognition_data.get('filename', 'unknown')
    
    log(f"å¤„ç†æ–‡æ¡£: {filename} ({doc_id})", "INFO")
    
    try:
        # æå–æ–‡æœ¬
        recognition = recognition_data.get('recognition', {})
        extracted_text = ''
        
        if recognition.get('type') == 'pdf' and recognition.get('pages'):
            texts = []
            for page in recognition.get('pages', []):
                if page.get('text'):
                    texts.append(page['text'])
            extracted_text = '\n'.join(texts).strip()
        elif recognition.get('text'):
            extracted_text = recognition.get('text', '')
        
        # å¦‚æœrecognition_dataç›´æ¥æœ‰textå­—æ®µï¼Œä¼˜å…ˆä½¿ç”¨
        if recognition_data.get('text'):
            extracted_text = recognition_data.get('text')
        
        if not extracted_text:
            log(f"æ— æ³•æå–æ–‡æœ¬å†…å®¹", "ERROR")
            update_status(doc_id, 'recognition_status', 'failed')
            update_status(doc_id, 'vector_status', 'failed')
            update_status(doc_id, 'graph_status', 'failed')
            return False
        
        log(f"æå–æ–‡æœ¬: {len(extracted_text)} å­—ç¬¦", "INFO")
        
        # ä¿å­˜æ–‡æœ¬åˆ°æ•°æ®åº“
        conn = get_db_connection()
        cur = conn.cursor()
        cur.execute("""
            UPDATE knowledge_documents 
            SET content_text = %s,
                recognition_status = 'completed',
                updated_at = NOW()
            WHERE id = %s
        """, (extracted_text, doc_id))
        conn.commit()
        cur.close()
        conn.close()
        log(f"æ–‡æœ¬å·²ä¿å­˜åˆ°æ•°æ®åº“", "SUCCESS")
        
        # å¹¶è¡Œå¤„ç†å‘é‡åŒ–å’Œå›¾è°±ï¼ˆå®é™…ä¸Šæ˜¯ä¸²è¡Œï¼Œä½†äº’ä¸å½±å“ï¼‰
        vector_success = process_vectorization(doc_id, extracted_text, kb_id, filename)
        graph_success = process_graph_extraction(doc_id, extracted_text, filename)
        
        if vector_success and graph_success:
            log(f"æ–‡æ¡£å¤„ç†å®Œæˆ: {filename}", "SUCCESS")
        elif vector_success or graph_success:
            log(f"æ–‡æ¡£éƒ¨åˆ†æˆåŠŸ: {filename}", "WARNING")
        else:
            log(f"æ–‡æ¡£å¤„ç†å¤±è´¥: {filename}", "ERROR")
        
        return True
        
    except Exception as e:
        log(f"å¤„ç†å¼‚å¸¸: {e}", "ERROR")
        traceback.print_exc()
        return False

def main():
    """ä¸»å‡½æ•°"""
    print("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  ğŸ¤– å¥å£®çš„æ–‡æ¡£è¯†åˆ«é˜Ÿåˆ—æ¶ˆè´¹è€…                                      â•‘
â•‘  ç›‘å¬: doc_recognition_queue                                    â•‘
â•‘  å¤„ç†: è¯†åˆ«ç»“æœ â†’ å‘é‡åŒ– + å›¾è°±æå–                               â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """, flush=True)
    
    # æ£€æŸ¥æœåŠ¡çŠ¶æ€
    log("æ£€æŸ¥æœåŠ¡çŠ¶æ€...", "INFO")
    
    # æ£€æŸ¥å‘é‡æœåŠ¡
    try:
        r = requests.get('http://localhost:8085/health', timeout=2)
        log("å‘é‡æœåŠ¡: è¿è¡Œä¸­", "SUCCESS")
    except:
        log("å‘é‡æœåŠ¡: æœªå“åº”ï¼ˆå°†ç»§ç»­å°è¯•ï¼‰", "WARNING")
    
    # æ£€æŸ¥GraphRAGæœåŠ¡
    try:
        r = requests.get('http://localhost:8081/health', timeout=2)
        log("GraphRAGæœåŠ¡: è¿è¡Œä¸­", "SUCCESS")
    except:
        log("GraphRAGæœåŠ¡: æœªå“åº”ï¼ˆå°†ç»§ç»­å°è¯•ï¼‰", "WARNING")
    
    # æ£€æŸ¥é˜Ÿåˆ—
    queue_len = redis_client.llen('doc_recognition_queue')
    if queue_len > 0:
        log(f"å‘ç° {queue_len} ä¸ªå¾…å¤„ç†ä»»åŠ¡", "INFO")
    
    processed_count = 0
    error_count = 0
    
    log("å¼€å§‹ç›‘å¬é˜Ÿåˆ—...", "INFO")
    
    while True:
        try:
            # é˜»å¡ç­‰å¾…ä»»åŠ¡
            result = redis_client.brpop('doc_recognition_queue', timeout=5)
            
            if result:
                _, task_json = result
                try:
                    recognition_data = json.loads(task_json)
                    
                    # å¤„ç†ä»»åŠ¡
                    success = process_recognition_result(recognition_data)
                    
                    if success:
                        processed_count += 1
                    else:
                        error_count += 1
                    
                    # å®šæœŸæŠ¥å‘Š
                    if (processed_count + error_count) % 10 == 0:
                        log(f"ç»Ÿè®¡: æˆåŠŸ={processed_count}, å¤±è´¥={error_count}", "INFO")
                        
                except json.JSONDecodeError as e:
                    log(f"æ— æ•ˆçš„JSONæ•°æ®: {e}", "ERROR")
                    error_count += 1
                except Exception as e:
                    log(f"å¤„ç†å¤±è´¥: {e}", "ERROR")
                    traceback.print_exc()
                    error_count += 1
                    
        except KeyboardInterrupt:
            log(f"åœæ­¢å¤„ç†", "INFO")
            log(f"æœ€ç»ˆç»Ÿè®¡: æˆåŠŸ={processed_count}, å¤±è´¥={error_count}", "INFO")
            break
        except Exception as e:
            log(f"é˜Ÿåˆ—é”™è¯¯: {e}", "ERROR")
            time.sleep(5)

if __name__ == '__main__':
    main()